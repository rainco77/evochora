pipeline {
  # Automatically start services in startupSequence when ServiceManager initializes
  # Set to false for manual control via API (useful for development/testing)
  autoStart = true  # default: true

  # Setup to run all indexers and and all services including dummy services for testing
  #startupSequence = ["dummy-indexer-1", "dummy-indexer-2", "environment-indexer-1", "environment-indexer-2", "organism-indexer-1", "organism-indexer-2", "metadata-indexer", "metadata-persistence-service", "persistence-service-1", "persistence-service-2", "persistence-service-3", "simulation-engine"]
  
  # Standard setup to run and process a new simulation
  startupSequence = ["environment-indexer-1", "environment-indexer-2", "organism-indexer-1", "organism-indexer-2", "metadata-indexer", "metadata-persistence-service", "persistence-service-1", "persistence-service-2", "simulation-engine"]
  
  # Setup to index an existing runId, runId configuration below has to be enabled
  #startupSequence = ["environment-indexer-1", "environment-indexer-2", "organism-indexer-1", "organism-indexer-2"]
  
  # Setup to serve only as webserver for the last indexed runId
  #startupSequence = []
  
  # Optional: Specify a specific simulation run ID for post-mortem mode
  # If set, all indexers will process only this single run ID instead of discovering
  # the latest run from storage. Useful for:
  #   - Re-indexing a specific run after database corruption
  #   - Testing/debugging indexing of a particular simulation
  #   - Post-mortem analysis of completed simulations
  # If not set (or commented out), indexers discover the latest run from storage
  # based on their start time (only runs starting AFTER indexer launch are discovered)
  # Format: YYYYMMDD-HHmmssSS-<UUID> (e.g., 20251009-14302512-550e8400-e29b-41d4-a716-446655440000)
  #runId = "20251122-05061163-303236c7-7133-43ef-af47-7e9d5bc96864"
  
  # Shared database configuration for all H2-based resources
  # Can be referenced by ${pipeline.database} in resource options with selective overrides
  database {
    # JDBC URL for H2 database connection
    # Both H2TopicResource and H2Database use this for consistency
    # 
    # Format: jdbc:h2:<path_to_database_file>[;<parameters>]
    # The path should include the database filename (without .mv.db extension)
    # H2 will create <path>.mv.db and <path>.trace.db files
    # 
    # Important H2 Parameters:
    #   - MODE=PostgreSQL: Use PostgreSQL-compatible SQL (required for BYTEA, MERGE, etc.)
    #   - AUTO_SERVER=TRUE: Allow multiple processes (enables H2 Console alongside application)
    # 
    # Variable expansion is handled by PathExpansion.java:
    #   - ${user.home} → /home/username
    #   - ${java.io.tmpdir} → /tmp
    #   - ${HOME} → environment variable
    #
    # Addtional H2 tuning parameters:
    #   - CACHE_SIZE: in KB (default: 16384 KB = 16 MB, way too small for production!)
    #       Set to 512 MB (524288 KB) to limit H2's memory usage while maintaining good performance
    #       H2 will flush to disk when cache is full
    #   - AUTO_SERVER: Allow multiple processes (enables H2 Console alongside application)
    #   - DB_CLOSE_ON_EXIT: Close the database on exit (default: true)
    #     Set to FALSE to prevent automatic close during JVM shutdown
    #   - DB_CLOSE_DELAY: Delay closing the database (default: 0)
    #     Set to -1 to prevent automatic close when last connection closes (required for graceful shutdown)
    #   - COMPRESS=TRUE: Compress the database (default: false)
    jdbcUrl = "jdbc:h2:${user.home}/evochora/data/indexdb;MODE=PostgreSQL;DB_CLOSE_ON_EXIT=FALSE;DB_CLOSE_DELAY=-1;CACHE_SIZE=262144"
    #jdbcUrl = "jdbc:h2:${user.home}/evochora/data/evochora;MODE=PostgreSQL;AUTO_SERVER=TRUE;CACHE_SIZE=524288"
    # Note: AUTO_SERVER=TRUE is not compatible with DB_CLOSE_ON_EXIT=FALSE
    # For external DB client access use H2 TCP Server in node section

    # Database credentials (default: username=sa, password=empty)
    # Used by all H2-based resources (topics, index database, etc.)
    username = "sa"
    password = ""

    # Default connection pool settings (can be overridden per resource)
    maxPoolSize = 10  # Maximum number of database connections in the pool
    minIdle = 2       # Minimum number of idle connections maintained in the pool

    # Default metrics window (can be overridden per resource)
    metricsWindowSeconds = 60  # Time window for rate-based metrics calculation
  }

  resources {
    # JVM Memory Monitoring - exposes heap, thread, and runtime metrics
    # Provides live JVM statistics AND optional historical aggregates (max/avg)
    # Uses SlidingWindowCounter for O(1) historical tracking with minimal overhead
    jvm-memory {
      className = "org.evochora.datapipeline.resources.monitoring.JvmMemoryMonitor"
      options {
        # Time window for historical metrics (max/avg calculation)
        # Set to 0 to disable historical tracking (live metrics only)
        # Recommended: 60-300 seconds for production monitoring
        windowSeconds = 60
        
        # Background sampling interval (seconds)
        # Set to 0 = on-demand sampling (metrics sampled during API calls only)
        # Set to 1+ = background thread samples at regular intervals (more accurate)
        # Recommended: 1 for accurate historical data, 0 to minimize overhead
        sampleIntervalSeconds = 1
      }
    }

    # H2-based topic for pub/sub messaging between pipeline services
    # Provides persistent, multi-writer queuing with consumer groups and competing consumers
    batch-topic {
      className = "org.evochora.datapipeline.resources.topics.H2TopicResource"
      options = ${pipeline.database} {
        # Topic-specific options (not in shared database config)
        claimTimeout = 300  # Stuck message reassignment timeout in seconds (default: 300)
        jdbcUrl = "jdbc:h2:${user.home}/evochora/data/topicdb;MODE=PostgreSQL;DB_CLOSE_ON_EXIT=FALSE;DB_CLOSE_DELAY=-1;CACHE_SIZE=262144"

        
        # All other options (jdbcUrl, username, password, maxPoolSize, minIdle, metricsWindowSeconds)
        # are inherited from ${pipeline.database} without modification
      }
    }

    # H2-based topic for metadata notifications (MetadataPersistenceService → MetadataIndexer)
    # Enables event-driven metadata indexing without storage polling
    metadata-topic {
      className = "org.evochora.datapipeline.resources.topics.H2TopicResource"
      options = ${pipeline.database} {
        # Stuck message reassignment timeout in seconds (default: 300)
        # For metadata-topic: single consumer group, so timeouts are unlikely
        # Set to same value as batch-topic for consistency
        claimTimeout = 300
        
        # All other options (jdbcUrl, username, password, maxPoolSize, minIdle, metricsWindowSeconds)
        # are inherited from ${pipeline.database} without modification
      }
    }

    index-database {
      className = "org.evochora.datapipeline.resources.database.H2Database"
      #All options inherited 1:1 from ${pipeline.database}
      options = ${pipeline.database} {
        # Environment storage strategy configuration
        # Defines how environment cell data is stored in the database
        # Different strategies trade off between storage size, query performance, and write performance
        h2EnvironmentStrategy {
          # SingleBlobStrategy: Stores all cells of a tick in a single BLOB
          # - Write: Fast (one BLOB per tick)
          # - Query: In-memory filtering after BLOB read (works for workstation-sized environments)
          # - Storage: Efficient with compression (~1 MB per tick @ 1000×1000, 50% occupancy)
          # - Scalability: 1000×1000 environments, up to ~1-2M ticks (~1-2 TB storage)
          # 
          # Future alternatives (not yet implemented):
          # - SpatialIndexStrategy: Database-level spatial queries (10-100× larger environments)
          # - ChunkedBlobStrategy: Spatial chunks with parallel decompression
          className = "org.evochora.datapipeline.resources.database.h2.SingleBlobStrategy"
          
          options {
            # BLOB compression configuration
            # Environment BLOBs are typically 10-100 MB uncompressed per tick
            # ZSTD compression achieves 10-15× compression ratio with minimal CPU overhead
            compression {
              # Enable compression for environment BLOBs (highly recommended)
              # Disabled: ~10 MB per tick @ 1000×1000, 50% → ~10 TB for 1M ticks
              # Enabled:  ~1 MB per tick @ 1000×1000, 50% → ~1 TB for 1M ticks
              enabled = true
              
              # Compression codec: "zstd" (recommended) or "none"
              # ZSTD provides best balance of compression ratio and speed
              codec = "zstd"
              
              # Compression level (1-22, default: 3)
              # Trade-off: Higher level = better compression, slower speed
              # 
              # Level 1: Fast compression/decompression (~600 MB/s), ~10× ratio
              #   - Use for: Speed-critical scenarios, large environments with frequent queries
              #   - Storage cost: +50% vs Level 3 (1.5 MB vs 1 MB per tick)
              # 
              # Level 3: Balanced (default) (~400-500 MB/s), ~10-15× ratio
              #   - Use for: Most scenarios, good balance of speed and storage
              #   - Storage cost: Baseline (1 MB per tick @ 1000×1000, 50%)
              # 
              # Level 9: Better compression (~200-300 MB/s), ~15-20× ratio
              #   - Use for: Storage-constrained scenarios, archival
              #   - Storage cost: -25% vs Level 3, but +50-100% decompression time
              # 
              # Recommendation: Level 3 for production (optimal trade-off)
              level = 1
            }
          }

        # Organism runtime state compression configuration
        # Controls how the per-tick organism runtime state blob (runtime_state_blob)
        # is compressed before being written to the database.
        #
        # Design is analogous to environment compression:
        # - On write: codec is chosen from this config.
        # - On read: codec is detected from magic bytes in the blob header, independent
        #   of current configuration (see OrganismIndexer spec and CompressionCodecFactory).
        organismRuntimeStateCompression {
          compression {
            # Enable compression for organism runtime_state_blob.
            # Recommended: enabled=true for real simulations to keep index size manageable.
            enabled = true

            # Compression codec for organism runtime state:
            # - "zstd": Recommended (good ratio, fast)
            # - "none": Disable compression for debugging / low data volumes
            codec = "zstd"

            # Compression level (1-22, default: 3)
            # Recommendation: Level 3 for production (balanced speed/storage),
            # lower levels (1) for debugging or very high write throughput needs.
            level = 3
          }
        }
        }
      }
      
      # Database wrappers support URI parameters for per-service overrides:
      # Example: database = "db-env-write:index-database?metricsWindowSeconds=30"
      # Supported URI parameters:
      #   - metricsWindowSeconds: Override metrics window for this specific wrapper
    }
    #test-queue {
    #  className = "org.evochora.datapipeline.resources.queues.InMemoryBlockingQueue"
    #  options {
    #    capacity = 1000
    #    metricsWindowSeconds = 5
    #  }
    #}

    #consumer-dlq {
    #  className = "org.evochora.datapipeline.resources.queues.InMemoryDeadLetterQueue"
    #  options {
    #    capacity = 100
    #    primaryQueueName = "test-queue"
    #  }
    #}

    #consumer-idempotency-tracker {
    #  className = "org.evochora.datapipeline.resources.idempotency.InMemoryIdempotencyTracker"
    #  options {
    #    ttlSeconds = 60
    #    cleanupThresholdMessages = 10000
    #    cleanupIntervalSeconds = 60
    #  }
    #}

    tick-storage {
      className = "org.evochora.datapipeline.resources.storage.FileSystemStorageResource"
      options {
        # Required: Absolute path to root directory for storing batch files
        # Subdirectories are created automatically based on simulationRunId
        # Example structure: /path/to/storage/sim-123/batch_0000000000000000000_0000000000000000999.pb
        #
        # Supports variable expansion using ${VAR} syntax:
        #   - System properties: ${user.home}, ${java.io.tmpdir}
        #   - Environment variables: ${HOME}, ${USERPROFILE}, ${PROJECT_ROOT}
        #   - Multiple variables: ${user.home}/${PROJECT_NAME}/data
        #
        # Cross-platform examples:
        #   ${user.home}/evochora-data       # Recommended (works everywhere)
        #   ${java.io.tmpdir}/evochora       # Temp directory
        #   ${HOME}/evochora-data            # Linux/macOS
        #   ${USERPROFILE}\\evochora-data    # Windows
        rootDirectory = "${user.home}/evochora/raw"

        # Hierarchical folder organization for batch files
        # Organizes batches into folders based on tick ranges to prevent filesystem
        # bottlenecks from too many files in a single directory.
        #
        # Structure with [100000000, 100000]:
        #   simulationId/001/234/batch_0012340000_0012340999.pb.zst
        #                │   │    └─ Batch file (firstTick to lastTick)
        #                │   └─ Level 1: hundred-thousands within hundred-million
        #                └─ Level 0: hundred-millions (tick / 100,000,000)
        #
        # Examples:
        #   - Tick 12,345,678 → folder 000/123/
        #   - Tick 123,456,789 → folder 001/234/
        #   - Tick 5,000,000,000 → folder 050/000/
        #
        # File discovery: Indexers use paginated listBatchFiles() to discover files
        # efficiently without loading all filenames into memory (S3-compatible).
        # With 1000-tick batches: ~100 files/folder (ideal for S3 pagination).
        folderStructure {
          # Tick range divisors for each folder level (outermost to innermost)
          # Default [100000000, 100000] creates 3-digit folders at both levels
          # Supports up to 99.9 billion ticks (999 × 100M)
          # Each folder contains ~100-1000 batch files depending on batch size
          levels = [100000000, 100000]
        }

        # Compression configuration for storage
        # Compression dramatically reduces disk space and I/O at minimal CPU cost.
        # Measured performance: 145-155x compression ratio, ~5% CPU overhead.
        #
        # Three configuration scenarios:
        #   1. enabled = true: Compression active (CURRENT)
        #   2. enabled = false: Explicitly disabled, no compression
        #   3. Missing section: No compression, backward compatible
        #
        # For a 1000x1000 simulation with 100 organisms:
        #   - Uncompressed: ~3.6 MB per tick → 3.6 GB per 1000 ticks
        #   - Compressed:   ~25 KB per tick → 25 MB per 1000 ticks (145x reduction)
        compression {
          enabled = true
          codec = "zstd"  # Recommended: excellent ratio + speed balance
          level = 3       # Optional, default: 3
                          # Range: 1-22
                          # Level 1: Fastest (400-500 MB/s), ~80x ratio
                          # Level 3: Balanced (400-500 MB/s), ~145x ratio (default)
                          # Level 9: Better ratio (200-300 MB/s), ~160x ratio
                          # Level 19-22: Max ratio (very slow), diminishing returns
        }
        #
        # Notes:
        #   - Compressed files use .zst extension (e.g., batch_000_999.pb.zst)
        #   - Backward compatible: Can read old uncompressed .pb files
        #   - Decompression is automatic and transparent
        #
        # Cross-platform support:
        #   - Windows x64: ✓ Works (bundled native library)
        #   - Linux x64/ARM64: ✓ Works (bundled native library)
        #   - macOS x64/ARM64: ✓ Works (bundled native library)
        #   - Docker (glibc): ✓ Works with ubuntu, debian, amazonlinux, etc.
        #   - Docker (musl/Alpine): ⚠ Requires additional setup (see below)
        #
        # Alpine Linux Docker Setup:
        #   Alpine uses musl libc instead of glibc. Two options:
        #
        #   Option A: Install glibc compatibility layer (recommended)
        #     FROM alpine:latest
        #     RUN apk add --no-cache gcompat
        #     # zstd-jni will use bundled library with gcompat
        #
        #   Option B: Install native zstd package
        #     FROM alpine:latest
        #     RUN apk add --no-cache zstd-jni zstd-libs
        #     # Uses Alpine's native zstd implementation
        #
        #   Option C: Use glibc-based base image (easiest)
        #     FROM ubuntu:22.04
        #     # or FROM debian:bookworm
        #     # or FROM amazoncorretto:21-alpine3.19 (includes gcompat)
        #     # Works immediately without additional packages
      }
    }

    persistence-dlq {
      className = "org.evochora.datapipeline.resources.queues.InMemoryDeadLetterQueue"
      options {
        # Maximum number of failed batches the DLQ can hold (default: 100)
        capacity = 100

        # Logical name of the primary queue being protected (for logging/monitoring)
        primaryQueueName = "tick-queue"
      }
    }

    persistence-idempotency {
      className = "org.evochora.datapipeline.resources.idempotency.InMemoryIdempotencyTracker"
      options {
        # Maximum number of keys to track (hard limit, FIFO eviction via ring buffer)
        # Window size (how far back duplicates are detected):
        #   At 100k TPS: 5M keys = 50 second window
        #   At 163k TPS: 5M keys = 30 second window
        #   At 200k TPS: 5M keys = 25 second window
        #   At 100k TPS: 50M keys = 500 second window (8.3 minutes)
        #
        # Memory usage (Long keys, optimized with Ring Buffer + HashSet):
        #   5M keys ≈ 160 MB RAM (~32 bytes per entry, 50% less than old LinkedHashSet)
        #   10M keys ≈ 320 MB RAM (~32 bytes per entry)
        #   50M keys ≈ 1.6 GB RAM (~32 bytes per entry)
        #
        # Note: Guaranteed memory limit, no growth beyond maxKeys!
        # Old keys are automatically evicted in FIFO order when limit is reached.
        maxKeys = 5000000

        # Initial HashSet capacity (default: maxKeys)
        # Set equal to maxKeys to avoid expensive rehashing during warmup.
        # Tradeoff: Uses full memory immediately vs. gradual growth with rehashing cost.
        initialCapacity = 5000000
      }
    }

    #tick-queue-null {
    #  # TEST: Using NullQueue (/dev/null) to measure pure simulation performance
    #  className = "org.evochora.datapipeline.resources.queues.NullQueue"
    #  options {
    #  }
    #
    #  # Original configuration (for comparison):
    #  # className = "org.evochora.datapipeline.resources.queues.InMemoryBlockingQueue"
    #  # options {
    #  #   capacity = 10000
    #  # }
    #}

    tick-queue {
      className = "org.evochora.datapipeline.resources.queues.InMemoryBlockingQueue"
      options {
        capacity = 5000
        # TEST: Disable all timestamp tracking inside the queue
        #disableTimestamps = true

        # Coalescing delay for competing consumers (milliseconds)
        # When queue is empty and producer is slow, wait this long after receiving
        # first tick to accumulate more ticks, improving batch sizes.
        # The drainLock GUARANTEES non-overlapping consecutive batch ranges.
        # Recommended: 100-200ms for slow producers with competing consumers
        coalescingDelayMs = 500
      }
    }

    metadata-queue {
      className = "org.evochora.datapipeline.resources.queues.InMemoryBlockingQueue"
      options {
        capacity = 10
      }
    }

    #consumer-dlq {
    #  className = "org.evochora.datapipeline.resources.queues.InMemoryDeadLetterQueue"
    #  options {
    #    capacity = 100
    #    primaryQueueName = "tick-queue"
    #  }
    #}

    #consumer-idempotency-tracker {
    #  className = "org.evochora.datapipeline.resources.idempotency.InMemoryIdempotencyTracker"
    #  options {
    #    ttlSeconds = 3600
    #    cleanupThresholdMessages = 100
    #    cleanupIntervalSeconds = 60
    #  }
    #}

    # Optional idempotency tracker for indexers (performance optimization)
    # Skips storage reads for duplicate batches after Topic redelivery
    # Note: MERGE statements already guarantee correctness - this is purely for performance
    index-idempotency {
      className = "org.evochora.datapipeline.resources.idempotency.InMemoryIdempotencyTracker"
      options {
        # Maximum number of batch IDs to track (hard limit, FIFO eviction via ring buffer)
        # Recommended: 5M keys ≈ 160 MB RAM (~32 bytes per entry)
        maxKeys = 5000000

        # Initial HashSet capacity (default: maxKeys)
        initialCapacity = 5000000
      }
    }

    # Shared retry tracker for indexers (optional, poison message handling)
    # Tracks retry counts across competing consumers for Dead Letter Queue (DLQ) functionality
    # Defense in Depth: Active cleanup + FIFO ring buffer ensures bounded memory
    indexer-retry {
      className = "org.evochora.datapipeline.resources.retry.InMemoryRetryTracker"
      options {
        # Maximum number of messages to track before FIFO eviction (default: 100000)
        # This is a SAFETY NET - normal operation removes entries immediately via cleanup
        #
        # Memory usage calculation (per entry: ~232 bytes):
        #   100,000 entries × 232 bytes = ~23 MB (worst-case maximum)
        #   50,000 entries × 232 bytes = ~11.6 MB
        #   200,000 entries × 232 bytes = ~46 MB
        #
        # Normal operation (99% success rate):
        #   - Entries removed immediately after success/DLQ → RAM: ~0 MB
        #   - Only active retries tracked (typically <1000) → RAM: ~0.23 MB
        #
        # When to adjust:
        #   - Increase if you expect many concurrent failures (e.g., storage outage)
        #   - Decrease to save RAM if poison messages are extremely rare
        #   - Recommended: Keep at 100k for safety (only 23 MB worst-case)
        #
        # FIFO protection: Oldest entries automatically evicted when limit reached
        # Even if cleanup fails (bug), memory is bounded (no memory leak possible!)
        maxKeys = 100000

        # Initial capacity for internal HashMaps (default: maxKeys)
        # Set equal to maxKeys to avoid expensive rehashing during warmup
        #
        # Tradeoff:
        #   - maxKeys value: Full memory allocated immediately, no rehashing overhead
        #   - Lower value: Gradual memory growth, but rehashing causes CPU spikes
        #
        # Recommendation: Keep equal to maxKeys for predictable performance
        initialCapacity = 100000
      }
    }

    # Shared DLQ for indexers (optional, poison message handling)
    # Receives poison messages that fail repeatedly after maxRetries
    # All competing consumers write to the same DLQ for centralized monitoring
    indexer-dlq {
      className = "org.evochora.datapipeline.resources.queues.InMemoryDeadLetterQueue"
      options {
        # Maximum number of poison messages the DLQ can hold (default: 10000)
        # When DLQ is full, put() throws InterruptedException → indexer enters ERROR state
        #
        # Memory usage calculation (per message: ~2-10 KB depending on batch size):
        #   10,000 messages × ~5 KB avg = ~50 MB
        #   1,000 messages × ~5 KB avg = ~5 MB
        #   100,000 messages × ~5 KB avg = ~500 MB
        #
        # When to adjust:
        #   - Increase if you expect many poison messages (e.g., corrupted storage)
        #   - Decrease to fail-fast if DLQ should never grow large
        #   - Recommended: 10k for most scenarios (monitor dlq_size metric)
        #
        # What happens when full:
        #   - Indexer can't write to DLQ → InterruptedException thrown
        #   - Indexer enters ERROR state → visible in monitoring
        #   - Operator must drain DLQ or increase capacity
        capacity = 10000

        # Logical name of the primary topic this DLQ serves (for monitoring/logging)
        # Appears in DLQ metrics and error messages for easier troubleshooting
        # No functional impact, purely for observability
        primaryQueueName = "batch-topic"
      }
    }
  }

  services {

    #########################################################
    # Common Services Options                               #
    #########################################################
    # The following options can be configured in any service's options {} block:
    #
    #   # Maximum time to wait for service shutdown cleanup (finally-block completion)
    #   # Increase for services with large flush operations
    #   # Example: shutdownTimeout = 10
    #   shutdownTimeout = 5 # (integer, seconds, default: 5)
    #    


    metadata-persistence-service {
      className = "org.evochora.datapipeline.services.MetadataPersistenceService"
      resources {
        # Required: Input queue for SimulationMetadata messages
        input = "queue-in:metadata-queue"

        # Required: Storage backend for writing metadata
        # Uses same storage as tick data to keep metadata and ticks together
        storage = "storage-write:tick-storage"

        # Required: Topic for metadata notifications (enables event-driven indexing)
        topic = "topic-write:metadata-topic"

        # Optional: Dead letter queue for failed metadata (shared with PersistenceService)
        dlq = "queue-out:persistence-dlq"
      }
      options {
        # Maximum retry attempts for transient write failures (default: 3)
        maxRetries = 3

        # Base delay in milliseconds for exponential backoff retry logic (default: 1000)
        # Retry delays follow pattern: base, base*2, base*4, ... (capped at 60000ms)
        retryBackoffMs = 1000
      }
    }

    #########################################################
    # Indexer Section                                       #
    #########################################################

    metadata-indexer {
      className = "org.evochora.datapipeline.services.indexers.MetadataIndexer"
      resources {
        # Required: Storage backend for reading metadata files
        # Must be the same storage where MetadataPersistenceService writes
        storage = "storage-read:tick-storage"

        # Required: Database for writing indexed metadata
        # Uses H2Database with connection pooling via HikariCP
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        database = "db-meta-write:index-database"

        # Required: Topic for metadata notifications (event-driven indexing)
        # Subscribes to metadata-topic with dedicated consumer group
        topic = "topic-read:metadata-topic?consumerGroup=metadata"
      }
      options {
        # Maximum time in milliseconds to wait for metadata notification (default: 30000)
        # MetadataPersistenceService publishes notification immediately after storage write
        # If notification doesn't arrive within this timeout, indexer enters ERROR state
        # This timeout should account for MetadataPersistenceService processing time
        # Recommended: 30000ms (30 seconds) for most scenarios
        topicPollTimeoutMs = 30000

        # Inherits from central pipeline.runId (if set)
        # If pipeline.runId not set → automatic discovery from storage
        # Can be overridden here for indexer-specific post-mortem mode
        runId = ${?pipeline.runId}
      }
    }

    environment-indexer-1 {
      className = "org.evochora.datapipeline.services.indexers.EnvironmentIndexer"
      
      resources {
        # Required: Storage backend for reading TickData batches
        # Must be the same storage where PersistenceService writes batches
        # Reads batch files published by PersistenceService after topic notification
        storage = "storage-read:tick-storage"
        
        # Required: Database for reading simulation metadata
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        metadata = "db-meta-read:index-database"

        # Required: Database for writing environment cell data
        # Uses write-specific wrapper (IEnvironmentDataWriter) with O(1) metrics
        # Writes cells to environment_ticks table via SingleBlobStrategy (BYTEA format)
        # MERGE ensures 100% idempotency on tick_number (duplicate batches are safe)
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        database = "db-env-write:index-database"

        # Required: Topic for batch notifications (event-driven indexing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group allows competing consumers (multiple EnvironmentIndexer instances)
        # Each batch is processed by exactly ONE consumer in the group
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=environment"
        
        # Optional: Idempotency tracker for performance optimization
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        #
        # Memory usage: See index-idempotency resource (5M keys ≈ 160 MB RAM)
        #
        # Requires: index-idempotency resource (see resources section above)
        # If resource not configured, idempotency component is NOT created (graceful degradation)
        idempotency = "idempotency:index-idempotency"
        
        # Optional: Retry tracker for DLQ functionality
        # Tracks retry counts across competing consumers for Dead Letter Queue (DLQ)
        # Shares retry state between all EnvironmentIndexer instances
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        retryTracker = "retry-tracker:indexer-retry"
        
        # Optional: Dead letter queue for poison message handling
        # Receives batches that fail repeatedly after maxRetries attempts
        # Prevents corrupted/poison batches from blocking the entire pipeline
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        dlq = "dlq:indexer-dlq"
      }
      
      options {
        # ===== Run ID Configuration =====
        
        # Optional: Explicit simulation run ID for post-mortem mode (default: auto-discovery)
        # If not set, indexer discovers the latest run from storage automatically
        # Only runs starting AFTER indexer launch are discovered
        #
        # Use cases for explicit runId:
        #   - Re-indexing a specific run after database corruption
        #   - Testing/debugging indexing of a particular simulation
        #   - Post-mortem analysis of completed simulations
        #
        # Format: YYYYMMDD-HHmmssSS-<UUID> (e.g., 20251020-143025-550e8400-e29b-41d4-a716-446655440000)
        #
        # Inherits from central pipeline.runId if set
        # Can be overridden here for indexer-specific post-mortem mode
        runId = ${?pipeline.runId}
        
        # ===== Metadata Reading Component =====
        
        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # After MetadataIndexer writes metadata, this indexer detects it within this interval
        #
        # Trade-offs:
        #   - Lower values (100-500ms): Faster detection, higher database load
        #   - Higher values (2000-5000ms): Lower database load, slower detection
        #
        # Recommended values:
        #   - Local database (same machine): 1000ms (default)
        #   - Network database (cloud): 2000-5000ms
        #   - Development/testing: 100ms for fast feedback
        metadataPollIntervalMs = 1000
        
        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        # Should account for:
        #   - MetadataIndexer startup time
        #   - MetadataIndexer processing time
        #   - MetadataPersistenceService delays
        #   - Potential retries
        #
        # Recommended values:
        #   - Production: 300000ms (5 minutes, default) - tolerant of delays
        #   - Testing: 30000ms (30 seconds) - fail-fast for quick feedback
        #   - Development: 10000ms (10 seconds) - very fast feedback
        metadataMaxPollDurationMs = 300000
        
        # ===== Batch Processing =====
        
        # Maximum time in milliseconds to wait for topic messages (default: auto-set to flushTimeoutMs)
        # When buffering is enabled, this is automatically set to flushTimeoutMs
        # Manual configuration only needed if buffering is disabled
        #
        # Why auto-set:
        #   - topic.poll() must timeout before flush timeout to allow buffer flush
        #   - Ensures buffered ticks are flushed even if no new batches arrive
        #   - Prevents indefinite waiting when simulation ends mid-buffer
        #
        # Default: Same as flushTimeoutMs (see below)
        # Only uncomment to override auto-setting:
        # topicPollTimeoutMs = 5000
        
        # ===== Tick Buffering Component =====
        
        # Number of ticks to buffer before flushing to database (default: 1000)
        # All buffered ticks are written in ONE JDBC batch with ONE commit
        #
        # Trade-offs:
        #   - Larger batches (5000+): Higher throughput, more memory, longer latency
        #   - Smaller batches (100-500): Lower latency, more database round-trips, lower throughput
        #   - Balanced (1000): Good for most scenarios
        #
        # Memory calculation (100×100 environment, 10% occupancy):
        #   - 1000 ticks × 1000 cells × ~100 bytes = ~100 MB in memory
        #   - 5000 ticks × 5000 cells × ~100 bytes = ~2.5 GB in memory
        #
        # Performance impact:
        #   - insertBatchSize=1000 → ~1000 cells/tick × 1000 ticks = 1M cells per commit
        #   - insertBatchSize=100 → 10× more commits for same data (slower)
        #
        # Recommended values:
        #   - Production (high throughput): 1000-5000
        #   - Development (low latency): 100-500
        #   - Testing: 10-100
        insertBatchSize = 1000
        
        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Prevents data loss if simulation ends before buffer fills up
        #
        # Trade-offs:
        #   - Lower values (1000ms): Lower latency, more partial flushes, slightly lower throughput
        #   - Higher values (10000ms): Higher latency, fewer partial flushes, slightly higher throughput
        #
        # Important: topicPollTimeoutMs is automatically set to this value
        # This ensures topic.poll() times out before flush, allowing periodic buffer flushing
        #
        # Recommended values:
        #   - Production: 5000ms (5 seconds, default) - balanced
        #   - Low-latency: 1000ms (1 second) - faster persistence
        #   - High-throughput: 10000ms (10 seconds) - fewer commits
        flushTimeoutMs = 5000
        
        # ===== DLQ Component =====
        
        # Maximum retry attempts before moving batch to DLQ (default: 3)
        # After this many failures across ALL competing consumers, batch goes to DLQ
        #
        # How it works:
        #   1. Batch fails processing → retry count incremented (shared across all consumers)
        #   2. Retry count > maxRetries → batch moved to DLQ, topic ACKed
        #   3. Pipeline continues (poison message no longer blocks)
        #
        # Retry counting is shared via IRetryTracker resource (thread-safe):
        #   - Consumer A fails → count=1
        #   - Consumer B fails (same batch, after reassignment) → count=2
        #   - Consumer C fails → count=3 → DLQ (if maxRetries=3)
        #
        # Different indexers can have different policies:
        #   - DummyIndexer: maxRetries=3 (fail-fast for testing)
        #   - EnvironmentIndexer: maxRetries=5 (more tolerant for production data)
        #   - Custom indexer: maxRetries=1 (zero tolerance for corruption)
        #
        # Note: Requires retryTracker and dlq resources to be configured (see resources above)
        # If resources not configured, DLQ component is NOT created (graceful degradation)
        #
        # Recommended values:
        #   - Production: 5 (tolerant of transient issues)
        #   - Testing: 3 (fail-fast)
        #   - Zero-tolerance: 1 (immediate DLQ on first failure)
        maxRetries = 5

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }

    environment-indexer-2 {
      className = "org.evochora.datapipeline.services.indexers.EnvironmentIndexer"

      resources {
        # Required: Storage backend for reading TickData batches
        # Must be the same storage where PersistenceService writes batches
        # Reads batch files published by PersistenceService after topic notification
        storage = "storage-read:tick-storage"

        # Required: Database for reading simulation metadata
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        metadata = "db-meta-read:index-database"

        # Required: Database for writing environment cell data
        # Uses write-specific wrapper (IEnvironmentDataWriter) with O(1) metrics
        # Writes cells to environment_ticks table via SingleBlobStrategy (BYTEA format)
        # MERGE ensures 100% idempotency on tick_number (duplicate batches are safe)
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        database = "db-env-write:index-database"

        # Required: Topic for batch notifications (event-driven indexing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group allows competing consumers (multiple EnvironmentIndexer instances)
        # Each batch is processed by exactly ONE consumer in the group
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=environment"

        # Optional: Idempotency tracker for performance optimization
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        #
        # Memory usage: See index-idempotency resource (5M keys ≈ 160 MB RAM)
        #
        # Requires: index-idempotency resource (see resources section above)
        # If resource not configured, idempotency component is NOT created (graceful degradation)
        idempotency = "idempotency:index-idempotency"

        # Optional: Retry tracker for DLQ functionality
        # Tracks retry counts across competing consumers for Dead Letter Queue (DLQ)
        # Shares retry state between all EnvironmentIndexer instances
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        retryTracker = "retry-tracker:indexer-retry"

        # Optional: Dead letter queue for poison message handling
        # Receives batches that fail repeatedly after maxRetries attempts
        # Prevents corrupted/poison batches from blocking the entire pipeline
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        dlq = "dlq:indexer-dlq"
      }

      options {
        # ===== Run ID Configuration =====

        # Optional: Explicit simulation run ID for post-mortem mode (default: auto-discovery)
        # If not set, indexer discovers the latest run from storage automatically
        # Only runs starting AFTER indexer launch are discovered
        #
        # Use cases for explicit runId:
        #   - Re-indexing a specific run after database corruption
        #   - Testing/debugging indexing of a particular simulation
        #   - Post-mortem analysis of completed simulations
        #
        # Format: YYYYMMDD-HHmmssSS-<UUID> (e.g., 20251020-143025-550e8400-e29b-41d4-a716-446655440000)
        #
        # Inherits from central pipeline.runId if set
        # Can be overridden here for indexer-specific post-mortem mode
        runId = ${?pipeline.runId}

        # ===== Metadata Reading Component =====

        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # After MetadataIndexer writes metadata, this indexer detects it within this interval
        #
        # Trade-offs:
        #   - Lower values (100-500ms): Faster detection, higher database load
        #   - Higher values (2000-5000ms): Lower database load, slower detection
        #
        # Recommended values:
        #   - Local database (same machine): 1000ms (default)
        #   - Network database (cloud): 2000-5000ms
        #   - Development/testing: 100ms for fast feedback
        metadataPollIntervalMs = 1000

        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        # Should account for:
        #   - MetadataIndexer startup time
        #   - MetadataIndexer processing time
        #   - MetadataPersistenceService delays
        #   - Potential retries
        #
        # Recommended values:
        #   - Production: 300000ms (5 minutes, default) - tolerant of delays
        #   - Testing: 30000ms (30 seconds) - fail-fast for quick feedback
        #   - Development: 10000ms (10 seconds) - very fast feedback
        metadataMaxPollDurationMs = 300000

        # ===== Batch Processing =====

        # Maximum time in milliseconds to wait for topic messages (default: auto-set to flushTimeoutMs)
        # When buffering is enabled, this is automatically set to flushTimeoutMs
        # Manual configuration only needed if buffering is disabled
        #
        # Why auto-set:
        #   - topic.poll() must timeout before flush timeout to allow buffer flush
        #   - Ensures buffered ticks are flushed even if no new batches arrive
        #   - Prevents indefinite waiting when simulation ends mid-buffer
        #
        # Default: Same as flushTimeoutMs (see below)
        # Only uncomment to override auto-setting:
        # topicPollTimeoutMs = 5000

        # ===== Tick Buffering Component =====

        # Number of ticks to buffer before flushing to database (default: 1000)
        # All buffered ticks are written in ONE JDBC batch with ONE commit
        #
        # Trade-offs:
        #   - Larger batches (5000+): Higher throughput, more memory, longer latency
        #   - Smaller batches (100-500): Lower latency, more database round-trips, lower throughput
        #   - Balanced (1000): Good for most scenarios
        #
        # Memory calculation (100×100 environment, 10% occupancy):
        #   - 1000 ticks × 1000 cells × ~100 bytes = ~100 MB in memory
        #   - 5000 ticks × 5000 cells × ~100 bytes = ~2.5 GB in memory
        #
        # Performance impact:
        #   - insertBatchSize=1000 → ~1000 cells/tick × 1000 ticks = 1M cells per commit
        #   - insertBatchSize=100 → 10× more commits for same data (slower)
        #
        # Recommended values:
        #   - Production (high throughput): 1000-5000
        #   - Development (low latency): 100-500
        #   - Testing: 10-100
        insertBatchSize = 1000

        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Prevents data loss if simulation ends before buffer fills up
        #
        # Trade-offs:
        #   - Lower values (1000ms): Lower latency, more partial flushes, slightly lower throughput
        #   - Higher values (10000ms): Higher latency, fewer partial flushes, slightly higher throughput
        #
        # Important: topicPollTimeoutMs is automatically set to this value
        # This ensures topic.poll() times out before flush, allowing periodic buffer flushing
        #
        # Recommended values:
        #   - Production: 5000ms (5 seconds, default) - balanced
        #   - Low-latency: 1000ms (1 second) - faster persistence
        #   - High-throughput: 10000ms (10 seconds) - fewer commits
        flushTimeoutMs = 5000

        # ===== DLQ Component =====

        # Maximum retry attempts before moving batch to DLQ (default: 3)
        # After this many failures across ALL competing consumers, batch goes to DLQ
        #
        # How it works:
        #   1. Batch fails processing → retry count incremented (shared across all consumers)
        #   2. Retry count > maxRetries → batch moved to DLQ, topic ACKed
        #   3. Pipeline continues (poison message no longer blocks)
        #
        # Retry counting is shared via IRetryTracker resource (thread-safe):
        #   - Consumer A fails → count=1
        #   - Consumer B fails (same batch, after reassignment) → count=2
        #   - Consumer C fails → count=3 → DLQ (if maxRetries=3)
        #
        # Different indexers can have different policies:
        #   - DummyIndexer: maxRetries=3 (fail-fast for testing)
        #   - EnvironmentIndexer: maxRetries=5 (more tolerant for production data)
        #   - Custom indexer: maxRetries=1 (zero tolerance for corruption)
        #
        # Note: Requires retryTracker and dlq resources to be configured (see resources above)
        # If resources not configured, DLQ component is NOT created (graceful degradation)
        #
        # Recommended values:
        #   - Production: 5 (tolerant of transient issues)
        #   - Testing: 3 (fail-fast)
        #   - Zero-tolerance: 1 (immediate DLQ on first failure)
        maxRetries = 5

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }

    organism-indexer-1 {
      className = "org.evochora.datapipeline.services.indexers.OrganismIndexer"

      resources {
        # Required: Storage backend for reading TickData batches with OrganismState entries
        # Must be the same storage where PersistenceService writes batches
        # Reads batch files published by PersistenceService after topic notification
        storage = "storage-read:tick-storage"

        # Required: Database for reading simulation metadata (environment shape etc.)
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata for the run
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        metadata = "db-meta-read:index-database"

        # Required: Database for writing organism data
        # Uses write-specific wrapper (IOrganismDataWriter) with O(1) metrics
        # Writes static data to organisms and per-tick data to organism_states
        # MERGE ensures idempotency on organism_id and (tick_number, organism_id)
        # runtime_state_blob is compressed according to organismRuntimeStateCompression
        database = "db-organism-write:index-database"

        # Required: Topic for batch notifications (event-driven indexing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group "organism" allows competing OrganismIndexer instances
        # Each batch is processed by exactly ONE consumer in the group
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=organism"

        # Optional: Idempotency tracker for performance optimization
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        idempotency = "idempotency:index-idempotency"

        # Optional: Retry tracker for DLQ functionality
        # Tracks retry counts across competing consumers for Dead Letter Queue (DLQ)
        # Shared between all indexers using indexer-retry
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        retryTracker = "retry-tracker:indexer-retry"

        # Optional: Dead letter queue for poison message handling
        # Receives batches that fail repeatedly after maxRetries attempts
        # Prevents corrupted/poison batches from blocking the entire pipeline
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        dlq = "dlq:indexer-dlq"
      }

      options {
        # ===== Run ID Configuration =====

        # Optional: Explicit simulation run ID for post-mortem mode (default: auto-discovery)
        # Inherits from central pipeline.runId if set
        # Can be overridden here for OrganismIndexer-specific post-mortem runs
        runId = ${?pipeline.runId}

        # ===== Metadata Reading Component =====

        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # After MetadataIndexer writes metadata, this indexer detects it within this interval
        metadataPollIntervalMs = 1000

        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        metadataMaxPollDurationMs = 300000

        # ===== Batch Processing =====

        # Maximum time in milliseconds to wait for topic messages (default: auto-set by AbstractBatchIndexer)
        # When buffering is enabled, topicPollTimeoutMs is automatically aligned with flushTimeoutMs
        # Manual configuration only needed if buffering behavior needs to be overridden
        # topicPollTimeoutMs = 5000

        # Number of ticks to buffer before flushing to database (default: 1000)
        # All buffered ticks are written in ONE JDBC batch with ONE commit
        #
        # Trade-offs:
        #   - Larger batches (5000+): Higher throughput, more memory, longer latency
        #   - Smaller batches (100-500): Lower latency, more database round-trips, lower throughput
        #   - Balanced (1000): Good for most scenarios
        #
        # For organism indexing, buffered size scales with number of organisms × ticks:
        #   - insertBatchSize=1000 → up to ~1000 ticks per commit (depending on data volume)
        #   - Smaller values reduce latency at the cost of more commits
        insertBatchSize = 1000

        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Prevents data loss if simulation ends before buffer fills up
        #
        # Important: topicPollTimeoutMs is automatically set to this value
        # This ensures topic.poll() times out before flush, allowing periodic buffer flushing
        #
        # Recommended values:
        #   - Production: 5000ms (5 seconds, default) - balanced
        #   - Low-latency: 1000ms (1 second) - faster persistence
        #   - High-throughput: 10000ms (10 seconds) - fewer commits
        flushTimeoutMs = 5000
      }
    }

    organism-indexer-2 {
      className = "org.evochora.datapipeline.services.indexers.OrganismIndexer"

      resources {
        # Required: Storage backend for reading TickData batches with OrganismState entries
        # Must be the same storage where PersistenceService writes batches
        # Reads batch files published by PersistenceService after topic notification
        storage = "storage-read:tick-storage"

        # Required: Database for reading simulation metadata (environment shape etc.)
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata for the run
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        metadata = "db-meta-read:index-database"

        # Required: Database for writing organism data
        # Uses write-specific wrapper (IOrganismDataWriter) with O(1) metrics
        # Writes static data to organisms and per-tick data to organism_states
        # MERGE ensures idempotency on organism_id and (tick_number, organism_id)
        # runtime_state_blob is compressed according to organismRuntimeStateCompression
        database = "db-organism-write:index-database"

        # Required: Topic for batch notifications (event-driven indexing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group "organism" allows competing OrganismIndexer instances
        # Each batch is processed by exactly ONE consumer in the group
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=organism"

        # Optional: Idempotency tracker for performance optimization
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        idempotency = "idempotency:index-idempotency"

        # Optional: Retry tracker for DLQ functionality
        # Tracks retry counts across competing consumers for Dead Letter Queue (DLQ)
        # Shared between all indexers using indexer-retry
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        retryTracker = "retry-tracker:indexer-retry"

        # Optional: Dead letter queue for poison message handling
        # Receives batches that fail repeatedly after maxRetries attempts
        # Prevents corrupted/poison batches from blocking the entire pipeline
        #
        # Required for DLQ: Both retryTracker AND dlq must be configured
        # If either is missing, DLQ component is NOT created (graceful degradation)
        dlq = "dlq:indexer-dlq"
      }

      options {
        # ===== Run ID Configuration =====

        # Optional: Explicit simulation run ID for post-mortem mode (default: auto-discovery)
        # Inherits from central pipeline.runId if set
        # Can be overridden here for OrganismIndexer-specific post-mortem runs
        runId = ${?pipeline.runId}

        # ===== Metadata Reading Component =====

        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # After MetadataIndexer writes metadata, this indexer detects it within this interval
        metadataPollIntervalMs = 1000

        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        metadataMaxPollDurationMs = 300000

        # ===== Batch Processing =====

        # Maximum time in milliseconds to wait for topic messages (default: auto-set by AbstractBatchIndexer)
        # When buffering is enabled, topicPollTimeoutMs is automatically aligned with flushTimeoutMs
        # Manual configuration only needed if buffering behavior needs to be overridden
        # topicPollTimeoutMs = 5000

        # Number of ticks to buffer before flushing to database (default: 1000)
        # All buffered ticks are written in ONE JDBC batch with ONE commit
        #
        # Trade-offs:
        #   - Larger batches (5000+): Higher throughput, more memory, longer latency
        #   - Smaller batches (100-500): Lower latency, more database round-trips, lower throughput
        #   - Balanced (1000): Good for most scenarios
        insertBatchSize = 1000

        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Prevents data loss if simulation ends before buffer fills up
        #
        # Important: topicPollTimeoutMs is automatically set to this value
        # This ensures topic.poll() times out before flush, allowing periodic buffer flushing
        #
        # Recommended values:
        #   - Production: 5000ms (5 seconds, default) - balanced
        #   - Low-latency: 1000ms (1 second) - faster persistence
        #   - High-throughput: 10000ms (10 seconds) - fewer commits
        flushTimeoutMs = 5000
      }
    }

    # DummyIndexer - Test indexer (AbstractBatchIndexer Foundation)
    # Purpose: Validates batch processing infrastructure (topic-based, storage-read, tick-by-tick)
    # Processes batches but only logs ticks (no database writes)
    dummy-indexer-1 {
      className = "org.evochora.datapipeline.services.indexers.DummyIndexer"
      
      resources {
        # Required: Storage backend for reading batch files
        # Used by AbstractBatchIndexer to read tick data after BatchInfo notification
        # Must match storage where PersistenceService writes batches
        storage = "storage-read:tick-storage"
        
        # Required: Database for reading indexed metadata
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        metadata = "db-meta-read:index-database"

        # Required: Topic for batch notifications (event-driven batch processing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group allows competing consumers (multiple DummyIndexer instances)
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=dummy"

        # Optional idempotency tracking (performance optimization)
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        #
        # Memory usage: See index-idempotency resource (5M keys ≈ 160 MB RAM)
        #
        # Requires: index-idempotency resource (see resources section above)
        # If resource not configured, idempotency component is NOT created (graceful degradation)
        idempotency = "idempotency:index-idempotency"

        # Optional DLQ for poison message handling
        # Moves batches to DLQ after maxRetries across ALL competing consumers
        # Prevents endless rotation of corrupted batches blocking pipeline
        #
        # Required resources (both must be configured to enable DLQ):
        #   - retryTracker: Shared counter across all competing consumers (thread-safe)
        #   - dlq: Dead letter queue for storing poison messages
        #
        # If either resource is missing, DLQ component is NOT created (graceful degradation)
        # Service continues without DLQ (poison messages will rotate indefinitely via Topic)
        #
        retryTracker = "retry-tracker:indexer-retry"
        dlq = "dlq:indexer-dlq"
      }
      
      options {
        # ===== Run ID Configuration =====
        
        # Inherits from central pipeline.runId (if set)
        # If pipeline.runId not set → automatic discovery from storage
        # Can be overridden here for indexer-specific post-mortem mode
        runId = ${?pipeline.runId}
        
        # ===== Metadata Reading Component =====
        
        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # Lower values = faster detection, higher database load
        # Higher values = lower database load, slower detection
        # Recommended: 1000ms for local databases, 2000-5000ms for network databases
        metadataPollIntervalMs = 1000
        
        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        # Should account for MetadataIndexer processing time + potential delays
        # Recommended: 300000ms (5 minutes) to account for slow MetadataIndexer and retries
        metadataMaxPollDurationMs = 300000
        
        # ===== Batch Processing =====
        
        # Note: topicPollTimeoutMs is automatically set to flushTimeoutMs when buffering is enabled
        # Manual configuration only needed if buffering is disabled (uncomment below)
        # topicPollTimeoutMs = 5000
        
        # ===== Tick Buffering Component =====
        
        # Number of ticks to buffer before flushing to database (default: 1000)
        # Trade-off: Larger batches = higher throughput, more memory, longer latency
        # Smaller batches = lower latency, more database round-trips
        # Recommended: 1000 for production, 100 for low-latency scenarios
        insertBatchSize = 1000
        
        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Note: topicPollTimeoutMs is automatically set to this value (no manual config needed)
        # Recommended: 5000ms (5 seconds) for most scenarios
        flushTimeoutMs = 5000
        
        # ===== DLQ Component =====

        # Maximum retry attempts before moving batch to DLQ (optional)
        # After this many failures across ALL competing consumers, batch goes to DLQ
        #
        # How it works:
        #   1. Batch fails processing → retry count incremented (shared across all consumers)
        #   2. Retry count > maxRetries → batch moved to DLQ, topic ACKed
        #   3. Pipeline continues (poison message no longer blocks)
        #
        # Retry counting is shared via IRetryTracker resource (thread-safe):
        #   - Consumer A fails → count=1
        #   - Consumer B fails (same batch, after reassignment) → count=2
        #   - Consumer C fails → count=3 → DLQ (if maxRetries=3)
        #
        # Different indexers can have different policies:
        #   - DummyIndexer: maxRetries=3 (fail-fast for testing)
        #   - EnvironmentIndexer: maxRetries=5 (more tolerant for production data)
        #   - Custom indexer: maxRetries=1 (zero tolerance for corruption)
        #
        # Note: Requires retryTracker and dlq resources to be configured (see resources above)
        # If resources not configured, DLQ component is NOT created (graceful degradation)
        # maxRetries = 3

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }

    # DummyIndexer - Test indexer (AbstractBatchIndexer Foundation)
    # Purpose: Validates batch processing infrastructure (topic-based, storage-read, tick-by-tick)
    # Processes batches but only logs ticks (no database writes)
    dummy-indexer-2 {
      className = "org.evochora.datapipeline.services.indexers.DummyIndexer"
      
      resources {
        # Required: Storage backend for reading batch files
        # Used by AbstractBatchIndexer to read tick data after BatchInfo notification
        # Must match storage where PersistenceService writes batches
        storage = "storage-read:tick-storage"
        
        # Required: Database for reading indexed metadata
        # Used by MetadataReadingComponent to poll for metadata availability
        # Polls database until MetadataIndexer has written metadata
        # Uses read-only wrapper (IMetadataReader) with O(1) metrics
        # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
        metadata = "db-meta-read:index-database"

        # Required: Topic for batch notifications (event-driven batch processing)
        # Subscribes to batch-topic published by PersistenceService
        # Consumer group allows competing consumers (multiple DummyIndexer instances)
        # Format: "usageType:resourceName?consumerGroup=group"
        topic = "topic-read:batch-topic?consumerGroup=dummy"

        # Optional idempotency tracking (performance optimization)
        # Skips storage reads for duplicate batches (e.g., after Topic redelivery)
        #
        # How it works:
        #   - Before reading from storage, checks if batch was already processed
        #   - If yes: Skip storage read, ACK immediately (performance optimization)
        #   - If no: Read from storage, process normally
        #
        # IMPORTANT: MERGE statements already guarantee correctness!
        #   - This is PURELY for performance (skip duplicate storage reads)
        #   - Even without idempotency, MERGE prevents duplicate database entries
        #   - Only enable if performance monitoring shows storage reads as bottleneck
        #
        # Memory usage: See index-idempotency resource (5M keys ≈ 160 MB RAM)
        #
        # Requires: index-idempotency resource (see resources section above)
        # If resource not configured, idempotency component is NOT created (graceful degradation)
        idempotency = "idempotency:index-idempotency"

        # Optional DLQ for poison message handling
        # Moves batches to DLQ after maxRetries across ALL competing consumers
        # Prevents endless rotation of corrupted batches blocking pipeline
        #
        # Required resources (both must be configured to enable DLQ):
        #   - retryTracker: Shared counter across all competing consumers (thread-safe)
        #   - dlq: Dead letter queue for storing poison messages
        #
        # If either resource is missing, DLQ component is NOT created (graceful degradation)
        # Service continues without DLQ (poison messages will rotate indefinitely via Topic)
        #
        retryTracker = "retry-tracker:indexer-retry"
        dlq = "dlq:indexer-dlq"
      }
      
      options {
        # ===== Run ID Configuration =====
        
        # Inherits from central pipeline.runId (if set)
        # If pipeline.runId not set → automatic discovery from storage
        # Can be overridden here for indexer-specific post-mortem mode
        runId = ${?pipeline.runId}
        
        # ===== Metadata Reading Component =====
        
        # Polling interval in milliseconds when waiting for metadata in database (default: 1000)
        # MetadataReadingComponent polls database.hasMetadata() until metadata is available
        # Lower values = faster detection, higher database load
        # Higher values = lower database load, slower detection
        # Recommended: 1000ms for local databases, 2000-5000ms for network databases
        metadataPollIntervalMs = 1000
        
        # Maximum time in milliseconds to wait for metadata before timeout (default: 300000)
        # If metadata doesn't appear in database within this timeout, indexer enters ERROR state
        # Should account for MetadataIndexer processing time + potential delays
        # Recommended: 300000ms (5 minutes) to account for slow MetadataIndexer and retries
        metadataMaxPollDurationMs = 300000
        
        # ===== Batch Processing =====
        
        # Note: topicPollTimeoutMs is automatically set to flushTimeoutMs when buffering is enabled
        # Manual configuration only needed if buffering is disabled (uncomment below)
        # topicPollTimeoutMs = 5000
        
        # ===== Tick Buffering Component =====
        
        # Number of ticks to buffer before flushing to database (default: 1000)
        # Trade-off: Larger batches = higher throughput, more memory, longer latency
        # Smaller batches = lower latency, more database round-trips
        # Recommended: 1000 for production, 100 for low-latency scenarios
        insertBatchSize = 1000
        
        # Maximum time in milliseconds to wait before flushing partial buffer (default: 5000)
        # Ensures buffered ticks are persisted within this timeout even if buffer not full
        # Note: topicPollTimeoutMs is automatically set to this value (no manual config needed)
        # Recommended: 5000ms (5 seconds) for most scenarios
        flushTimeoutMs = 5000
        
        # ===== DLQ Component =====

        # Maximum retry attempts before moving batch to DLQ (optional)
        # After this many failures across ALL competing consumers, batch goes to DLQ
        #
        # How it works:
        #   1. Batch fails processing → retry count incremented (shared across all consumers)
        #   2. Retry count > maxRetries → batch moved to DLQ, topic ACKed
        #   3. Pipeline continues (poison message no longer blocks)
        #
        # Retry counting is shared via IRetryTracker resource (thread-safe):
        #   - Consumer A fails → count=1
        #   - Consumer B fails (same batch, after reassignment) → count=2
        #   - Consumer C fails → count=3 → DLQ (if maxRetries=3)
        #
        # Different indexers can have different policies:
        #   - DummyIndexer: maxRetries=3 (fail-fast for testing)
        #   - EnvironmentIndexer: maxRetries=5 (more tolerant for production data)
        #   - Custom indexer: maxRetries=1 (zero tolerance for corruption)
        #
        # Note: Requires retryTracker and dlq resources to be configured (see resources above)
        # If resources not configured, DLQ component is NOT created (graceful degradation)
        # maxRetries = 3

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }

    #########################################################
    # Simulation Engine Service                             #
    #########################################################
    simulation-engine {
      className = "org.evochora.datapipeline.services.SimulationEngine"
      resources {
        tickData = "queue-out:tick-queue"
        metadataOutput = "queue-out:metadata-queue"
      }
      options {
        # Sampling interval: capture tick data every N ticks (1 = every tick)
        samplingInterval = 1

        # Time window in seconds for ticks_per_second calculation (default: 1)
        metricsWindowSeconds = 5

        # Optional: Pause simulation at specific ticks for debugging
        #pauseTicks = [100,2000]

        # Random seed for reproducible simulations (omit for random seed)
        seed = 44

        # Environment configuration
        environment {
          # World dimensions (2D: [width, height], 3D: [width, height, depth])
          # 
          # Performance optimization for HTTP API (SingleBlobStrategy):
          #   - If using SingleBlobStrategy for environment indexing, prefer power-of-2 widths
          #     (256, 512, 1024, 2048, 4096) for automatic bit-operations optimization
          #   - Power-of-2 width enables ~57% faster region filtering (7ms → 3ms for 31k cells)
          #   - Height and other dimensions can be any value (only width matters!)
          #   - Examples: [1024, 1000], [2048, 2500], [1024, 1024]
          #   - Non-power-of-2 widths still work correctly, just slightly slower
          shape = [800, 600]

          # Topology: "TORUS" for wraparound edges, anything else for bounded
          topology = "TORUS"
        }

        # Energy distribution strategies (multiple can be active simultaneously)
        energyStrategies = [
          {
            className = "org.evochora.runtime.worldgen.GeyserCreator"
            options {
              # Number of geysers to spawn
              count = 1000

              # Tick interval between eruptions
              interval = 100

              # Energy amount placed per eruption
              amount = 10000

              # Radius around placement that must be unowned by organisms
              safetyRadius = 3
            }
          },
          {
            className = "org.evochora.runtime.worldgen.SolarRadiationCreator"
            options {
              # Probability per execution to spawn energy (0.0 to 1.0)
              probability = 0.02
          
              # Energy amount placed when spawn succeeds
              amount = 5000
          
              # Radius around placement that must be unowned by organisms
              safetyRadius = 1
          
              # Number of independent spawn attempts per tick
              executionsPerTick = 1
            }
          }
        ]

        # Initial organisms to spawn at simulation start
        organisms = [
          {
            # Path to the assembly program file.
            program = "assembly/primordial/main.evo"
            #program = "assembly/examples/simple.evo"
            #program = "assembly/test/main.evo"            # nested procs:

            # Initial energy level of the organism.
            initialEnergy = 10000

            # Placement in the environment
            placement {
              # Position coordinates [x, y] for 2D or [x, y, z] for 3D
              positions = [375, 285]
            }
          }
        ]

        # Optional: User-defined metadata for experiment tracking
        metadata {
          experiment = "test-run"
          version = "1.0"
        }
      }
    }

    #tick-consumer {
    #  className = "org.evochora.datapipeline.services.DummyConsumerService"
    #  resources {
    #    input = "queue-in:tick-queue"
    #    idempotencyTracker = "consumer-idempotency-tracker"
    #    dlq = "queue-out:consumer-dlq"
    #  }
    #  options {
    #    #processingDelayMs = 50
    #    maxMessages = -1
    #  }
    #}


    #########################################################
    # Persistence Service                                   #
    #########################################################
    persistence-service-1 {
      className = "org.evochora.datapipeline.services.PersistenceService"
      resources {
        # Required: Input queue for TickData messages
        input = "queue-in:tick-queue"

        # Required: Storage backend for writing batches
        storage = "storage-write:tick-storage"

        # Required: Topic for batch notifications (enables event-driven indexing)
        topic = "topic-write:batch-topic"

        # Optional: Dead letter queue for failed batches
        dlq = "queue-out:persistence-dlq"

        # Optional: Idempotency tracker to detect duplicate ticks (bug detection)
        #idempotencyTracker = "persistence-idempotency"
      }
      options {
        # Maximum number of TickData messages to batch together (default: 1000)
        maxBatchSize = 1000

        # Maximum seconds to wait before flushing partial batch (default: 5)
        batchTimeoutSeconds = 5

        # Maximum retry attempts for transient write failures (default: 3)
        maxRetries = 3

        # Base delay in milliseconds for exponential backoff retry logic (default: 1000)
        # Retry delays follow pattern: base, base*2, base*4, ... (capped at 60000ms)
        retryBackoffMs = 1000

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }

    persistence-service-2 {
      className = "org.evochora.datapipeline.services.PersistenceService"
      resources {
        # Required: Input queue for TickData messages
        input = "queue-in:tick-queue"

        # Required: Storage backend for writing batches
        storage = "storage-write:tick-storage"

        # Required: Topic for batch notifications (enables event-driven indexing)
        topic = "topic-write:batch-topic"

        # Optional: Dead letter queue for failed batches
        dlq = "queue-out:persistence-dlq"

        # Optional: Idempotency tracker to detect duplicate ticks (bug detection)
        #idempotencyTracker = "persistence-idempotency"
      }
      options {
        # Maximum number of TickData messages to batch together (default: 1000)
        maxBatchSize = 1000

        # Maximum seconds to wait before flushing partial batch (default: 5)
        batchTimeoutSeconds = 5

        # Maximum retry attempts for transient write failures (default: 3)
        maxRetries = 3

        # Base delay in milliseconds for exponential backoff retry logic (default: 1000)
        # Retry delays follow pattern: base, base*2, base*4, ... (capped at 60000ms)
        retryBackoffMs = 1000

        # Maximum time to wait for service shutdown cleanup (finally-block completion)
        # Increase for services with large flush operations
        # Example: shutdownTimeout = 10
        shutdownTimeout = 15 # (integer, seconds, default: 5)
      }
    }


    #dummy-consumer {
    #  className = "org.evochora.datapipeline.services.DummyConsumerService"
    #  resources {
    #    input = "queue-in:test-queue"
    #    idempotencyTracker = "consumer-idempotency-tracker"
    #    dlq = "queue-out:consumer-dlq"
    #  }
    #  options {
    #    # Artificial delay per message in milliseconds (default: 0)
    #    processingDelayMs = 50
    #
    #    # Maximum messages to process before stopping, -1 for unlimited (default: -1)
    #    # Purpose: Limits total messages consumed for testing/demos
    #    # - For tests: Set to specific count (e.g., 100) → service stops after N messages
    #    # - For unlimited: Set to -1 (default) → service runs until manually stopped
    #    # - Memory safety: Prevents unbounded internal state growth in test services
    #    maxMessages = 100
    #
    #    # Time window in seconds for throughput calculation (default: 5)
    #    metricsWindowSeconds = 5
    #
    #    # Maximum processing attempts before sending to DLQ (default: 3)
    #    maxRetries = 3
    #
    #    # Whether to log received messages at DEBUG level (default: false)
    #    #logReceivedMessages = false
    #  }
    #}

    #dummy-producer {
    #  className = "org.evochora.datapipeline.services.DummyProducerService"
    #  resources {
    #    output = "queue-out:test-queue?metricsWindowSeconds=2"
    #  }
    #  options {
    #    # Milliseconds between messages (default: 1000)
    #    intervalMs = 100
    #
    #    # Maximum messages to send before stopping, -1 for unlimited (default: -1)
    #    # Purpose: Limits total messages produced for testing/demos
    #    # - For tests: Set to specific count (e.g., 100) → service stops after N messages
    #    # - For unlimited: Set to -1 (default) → service runs until manually stopped
    #    # - Memory safety: Prevents unbounded internal state growth in test services
    #    maxMessages = 100
    #
    #    # Time window in seconds for throughput calculation (default: 5)
    #    metricsWindowSeconds = 5
    #
    #    # Prefix for the message content (default: "Message")
    #    #messagePrefix = "Message"
    #  }
    #}

    #dummy-writer {
    #  className = "org.evochora.datapipeline.services.DummyWriterService"
    #  resources {
    #    storage = "storage-write:tick-storage"
    #  }
    #  options {
    #    # Milliseconds between write operations (default: 1000)
    #    intervalMs = 1000
    #
    #    # Number of messages per write batch (default: 10)
    #    messagesPerWrite = 10
    #
    #    # Maximum write operations before stopping, -1 for unlimited (default: -1)
    #    # Purpose: Limits total batches written for testing/demos
    #    # - For tests: Set to specific count (e.g., 100) → service stops after N writes
    #    # - For unlimited: Set to -1 (default) → service runs until manually stopped
    #    # - Memory safety: Prevents unbounded internal state growth in test services
    #    maxWrites = -1
    #
    #    # Key prefix for storage (default: "test")
    #    keyPrefix = "test"
    #  }
    #}

    #dummy-reader {
    #  className = "org.evochora.datapipeline.services.DummyReaderService"
    #  resources {
    #    storage = "storage-read:tick-storage"
    #  }
    #  options {
    #    # Milliseconds between polling cycles (default: 1000)
    #    intervalMs = 1000
    #
    #    # Key prefix for filtering files (default: "test")
    #    keyPrefix = "test"
    #
    #    # Enable data validation (default: true)
    #    validateData = true
    #
    #    # Maximum files to process before stopping, -1 for unlimited (default: -1)
    #    # Purpose: Limits total files processed for testing/demos
    #    # - For tests: Set to specific count (e.g., 100) → service stops after N files
    #    # - For unlimited: Set to -1 (default) → service runs until manually stopped
    #    # - Memory safety: Prevents unbounded processedFiles Set growth (CRITICAL!)
    #    #   Without this limit, processedFiles grows unbounded → memory leak
    #    maxFiles = -1
    #  }
    #}
  }
}

# The node block configures the server process itself
node {
  # Show ASCII art welcome message on startup (default: false)
  show-welcome-message = true

  # Defines all long-running processes the Node should start and manage
  processes {

    # Pipeline process - manages data pipeline services (simulation, persistence, etc.)
    # Must be defined BEFORE http process since http depends on it
    pipeline {
      className = "org.evochora.datapipeline.ServiceManagerProcess"
      options = ${pipeline}  # References the top-level pipeline configuration
    }

    # H2 Database Web Console - separate web interface for database access
    # Runs on its own port (default: 8082) independent of the main HTTP server
    # Security: Restricted to localhost by default
    h2-console {
      className = "org.evochora.node.processes.h2.H2ConsoleProcess"
      options {
        # Enable/disable the console (recommended: disable in production)
        enabled = true

        # Network configuration
        network {
          host = "localhost"  # Restrict to localhost for security
          port = 8082         # Standard H2 console port
        }

        # Database configuration reference (for displaying connection info in logs)
        # References the index-database config to show correct JDBC URL and credentials
        database = ${pipeline.resources.index-database.options}

        # Security: Allow connections from other computers (NOT recommended!)
        allowOthers = false

        # Enable SSL/TLS (requires SSL certificate configuration)
        useSSL = false

        # Enable H2 trace output for debugging
        trace = false
      }
    }

    # H2 TCP Server - allows external database clients (IntelliJ, DBeaver, etc.) to connect
    # This is an alternative to AUTO_SERVER=TRUE (which is incompatible with DB_CLOSE_ON_EXIT=FALSE)
    # Security: Restricted to localhost by default
    h2-tcp-server {
      className = "org.evochora.node.processes.h2.H2TcpServerProcess"
      options {
        # Enable/disable the TCP server (default: false = disabled, opt-in)
        # Set to true to allow external database client connections
        enabled = true

        # TCP port for database connections (default: 9092)
        tcpPort = 9092

        # Database configuration reference (for displaying correct JDBC URL in logs)
        # References the index-database config to build the TCP connection URL
        database = ${pipeline.resources.index-database.options}

        # Security: Allow connections from other computers (NOT recommended!)
        # false = localhost only (default, secure)
        # true = allow remote connections (security risk!)
        tcpAllowOthers = false
      }
    }

    # Logical name for the HTTP server process
    http {
      className = "org.evochora.node.processes.http.HttpServerProcess"

      # Declare dependency on pipeline process
      require = {
        serviceManager = "pipeline"
      }

      options {
        network {
          host = "localhost"
          port = 8081

          # Optional: Configure the Jetty thread pool for HTTP request handling
          # Threads are named "<processName>-http-<number>" for easier monitoring
          threadPool {
            minThreads = 8         # Minimum number of threads (default: 8)
            maxThreads = 200       # Maximum number of threads (default: 200)
            idleTimeoutMs = 60000  # Idle timeout in milliseconds (default: 60000)
          }
        }

        # Database provider resource name for HTTP controllers
        # This resource provides IDatabaseReaderProvider for environment data access
        databaseProviderResourceName = "index-database"

        # All routes are defined within the HttpServerProcess's options
        routes {
          # The nesting of objects defines the URL paths.
          # Actions ($controller, $static) are defined by keys prefixed with '$'.
          pipeline {
            # ACTION: Serves static UI files at the base path "/pipeline"
            # The static file handler serves from the classpath. A directory named
            # 'web/pipeline-control' should exist in 'src/main/resources'.
            #"$static" = "/web/pipeline-control"

            # Builds the sub-path "/pipeline/api"
            api {
              # ACTION: Serves a controller at "/pipeline/api"
              "$controller" {
                className = "org.evochora.node.processes.http.api.pipeline.PipelineController"
                options {}
              }
            }
          }

          # Environment data visualization API
          # Provides HTTP endpoints for the web-based visualizer client
          visualizer {
            # ACTION: Serves static UI files at the base path "/visualizer"
            # The static file handler serves from the classpath. A directory named
            # 'web/visualizer' should exist in 'src/main/resources'.
            "$static" = "web/visualizer"
            
            api {
              # Environment controller at "/visualizer/api/environment/{tick}"
              # Returns cell data for a specific tick and optional region query parameter
              environment {
                "$controller" {
                  className = "org.evochora.node.processes.http.api.visualizer.EnvironmentController"
                  options {
                    # HTTP caching configuration for environment endpoint
                    # Controls browser and proxy caching behavior for environment cell data
                    cache {
                      # Enable/disable HTTP caching (default: false = no caching)
                      # When enabled, browsers and proxies cache responses according to maxAge
                      # When disabled, responses are marked as non-cacheable (no-cache, no-store)
                      # 
                      # Use cases:
                      #   - enabled = false: Always fetch fresh data (development, debugging)
                      #   - enabled = true: Cache responses for performance (production)
                      enabled = false
                      
                      # Maximum age in seconds for cached responses (default: 0)
                      # Only relevant when enabled = true
                      # 
                      # Examples:
                      #   - maxAge = 0: Cache disabled (same as enabled = false)
                      #   - maxAge = 60: Cache for 1 minute
                      #   - maxAge = 3600: Cache for 1 hour
                      #   - maxAge = 31536000: Cache for 1 year (immutable data)
                      # 
                      # Note: Even with maxAge > 0, must-revalidate ensures browsers
                      #       send If-None-Match header for ETag validation
                      maxAge = 0
                      
                      # Enable/disable ETag validation (default: false = no ETag)
                      # When enabled, server generates ETag header based on runId
                      # Browsers send If-None-Match header on subsequent requests
                      # Server responds with 304 Not Modified if ETag matches (saves bandwidth)
                      # 
                      # ETag format: "runId" (e.g., "20251020-143025-550e8400...")
                      # 
                      # Use cases:
                      #   - useETag = false: No validation, always return full response
                      #   - useETag = true: Conditional requests, 304 responses when data unchanged
                      # 
                      # Recommended: Enable for production to reduce bandwidth
                      # Note: ETag changes when runId changes, forcing cache invalidation
                      useETag = false
                    }
                  }
                }
              }
              
              # Simulation metadata and tick information at "/visualizer/api/simulation"
              # Provides two endpoints:
              #   - GET /visualizer/api/simulation/metadata?runId=... (simulation metadata)
              #   - GET /visualizer/api/simulation/ticks?runId=... (tick range: minTick, maxTick)
              simulation {
                "$controller" {
                  className = "org.evochora.node.processes.http.api.visualizer.SimulationController"
                  options {
                    # HTTP caching configuration for simulation endpoints
                    # Each endpoint (metadata, ticks) can be configured independently
                    cache {
                      # Configuration for /metadata endpoint
                      metadata {
                        # Enable/disable HTTP caching (default: false = no caching)
                        enabled = false
                        
                        # Maximum age in seconds for cached responses (default: 0)
                        # Only relevant when enabled = true
                        maxAge = 0
                        
                        # Enable/disable ETag validation (default: false = no ETag)
                        # ETag format: "runId" (e.g., "20251020-143025-550e8400...")
                        # Changes when runId changes, forcing cache invalidation
                        useETag = false
                      }
                      
                      # Configuration for /ticks endpoint
                      ticks {
                        # Enable/disable HTTP caching (default: false = no caching)
                        enabled = false
                        
                        # Maximum age in seconds for cached responses (default: 0)
                        # Only relevant when enabled = true
                        maxAge = 0
                        
                        # Enable/disable ETag validation (default: false = no ETag)
                        # ETag format: "runId_maxTick" (e.g., "20251020-143025-550e8400..._1000")
                        # Changes when runId OR maxTick changes, forcing cache invalidation
                        useETag = false
                      }
                    }
                  }
                }
              }

              # Organism controller at "/visualizer/api/organisms/{tick}" and
              # "/visualizer/api/organisms/{tick}/{organismId}"
              # Provides organism summaries for a tick and detailed state for a specific organism.
              organisms {
                "$controller" {
                  className = "org.evochora.node.processes.http.api.visualizer.OrganismController"
                  options {
                    # HTTP caching configuration for organism endpoints.
                    # By default, caching is disabled to always reflect the latest indexed data.
                    cache {
                      # Configuration for tick-list endpoint: /visualizer/api/organisms/{tick}
                      organisms {
                        # Enable/disable HTTP caching for organism tick lists (default: false).
                        # When enabled, responses may be cached by clients/proxies for maxAge seconds.
                        enabled = false

                        # Maximum age in seconds for cached responses (only relevant when enabled=true).
                        maxAge = 0

                        # Enable/disable ETag validation for tick-list responses.
                        # ETag format: "runId_tick" (e.g., "20251020-143025-..._1234").
                        useETag = false
                      }

                      # Configuration for detail endpoint: /visualizer/api/organisms/{tick}/{organismId}
                      organismDetails {
                        # Enable/disable HTTP caching for organism detail responses (default: false).
                        enabled = false

                        # Maximum age in seconds for cached responses (only relevant when enabled=true).
                        maxAge = 0

                        # Enable/disable ETag validation for detail responses.
                        # ETag format: "runId_tick_organismId".
                        useETag = false
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

logging {
  format = "PLAIN"  # Can be "PLAIN" or "JSON". Defaults to JSON
  default-level = "INFO"  # Default log level for all loggers
  levels {
    # Specific logger levels - override the default for particular components
    #"org.evochora.datapipeline.ServiceManager" = "INFO"
    #"org.evochora.datapipeline.services.indexers.DummyIndexer" = "DEBUG"
    #"org.evochora.datapipeline.services.indexers.EnvironmentIndexer" = "DEBUG"
    #"org.evochora.datapipeline.services.indexers.OrganismIndexer" = "DEBUG"
    #"org.evochora.datapipeline.services.SimulationEngine" = "DEBUG"
    #"org.evochora.node.processes.http.HttpServerProcess" = "DEBUG"
    #"org.evochora.compiler" = "DEBUG"
  }
}