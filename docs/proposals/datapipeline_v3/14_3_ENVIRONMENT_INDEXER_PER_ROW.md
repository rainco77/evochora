# Data Pipeline V3 - Environment Indexer (Phase 14.3) - SingleBlobStrategy

## Overview

This document specifies the **EnvironmentIndexer** with **SingleBlobStrategy** (default storage strategy). This is the first indexer that writes structured data from storage into a database. It indexes environment cell states from TickData using a **single-blob-per-tick** storage model with BLOB serialization for compact storage.

## Goal

Implement EnvironmentIndexer with SingleBlobStrategy that:
- Reads TickData batches from storage (via topic notifications)
- Serializes cells to BLOB for compact storage
- Writes ticks to database with MERGE for idempotency (on tick_number)
- Supports dimension-agnostic BLOB format (1D to N-D)
- Enables competing consumers pattern
- Provides foundation for HTTP API queries (Phase 14.5)
- Achieves 1000√ó storage reduction compared to row-per-cell

## Success Criteria

Upon completion:
1. EnvironmentIndexer extends AbstractBatchIndexer with METADATA + BUFFERING components
2. Database schema created idempotently (environment_ticks table)
3. SingleBlobStrategy serializes cells to BLOB (protobuf format)
4. MERGE-based writes ensure 100% idempotency (on tick_number)
5. Sparse cell storage (only non-empty cells in BLOB)
6. Competing consumers work correctly (multiple indexer instances)
7. All tests pass (unit + integration)
8. Storage efficiency: ~1 GB for 10^6 ticks (vs. 5 TB with row-per-cell)

## Prerequisites

- Phase 14.2.6: AbstractBatchIndexer with TickBufferingComponent (completed)
- Phase 14.2.5: DummyIndexer as reference implementation (completed)
- AbstractDatabaseWrapper supports URI parameters (consistency with Storage/Queue resources)
- EnvironmentProperties extended with flatIndexToCoordinates() method
- **CellStateList protobuf message** added to tickdata_contracts.proto (used by SingleBlobStrategy)

## Architectural Design

### Component Architecture

EnvironmentIndexer follows the three-level indexer architecture:

```
AbstractIndexer (core: run discovery, schema management)
    ‚Üì
AbstractBatchIndexer (batch processing: topic loop, buffering, ACK tracking)
    ‚Üì
EnvironmentIndexer (only: prepareSchema() + flushTicks())
```

**Resources Required:**
- `storage` - IBatchStorageRead (read TickData batches)
- `topic` - ITopicReader<BatchInfo, ACK> (batch notifications)
- `metadata` - IMetadataReader (simulation metadata)
- `database` - IEnvironmentDataWriter (write environment cells)

**Components Used:**
- ‚úÖ MetadataReadingComponent (default)
- ‚úÖ TickBufferingComponent (default)
- ‚ùå IdempotencyComponent (not needed - MERGE provides idempotency)

### Data Flow

```
PersistenceService ‚Üí Storage (TickData batches)
                  ‚Üí Topic (BatchInfo notifications)
                         ‚Üì
                    [Topic Poll - Blocking]
                         ‚Üì
EnvironmentIndexer ‚Üí Read TickData from Storage
                  ‚Üí Extract CellState list
                  ‚Üí Buffer cells (TickBufferingComponent)
                  ‚Üí Flush ‚Üí flatIndex ‚Üí coordinates (EnvironmentProperties)
                         ‚Üí MERGE to database (100% idempotent)
                  ‚Üí ACK (only after ALL ticks of batch flushed)
```

### Idempotency Strategy

**100% idempotent via MERGE:**

```sql
MERGE INTO environment_ticks (tick_number, cells_blob)
KEY (tick_number)
VALUES (?, ?)
```

**Crash Recovery:**
```
1. Flush: 250 cells ‚Üí MERGE ‚Üí DB ‚úÖ
2. CRASH before ACK
3. Topic redelivery: same BatchInfo
4. Re-read storage ‚Üí same cells ‚Üí MERGE
   - Existing rows: UPDATE (noop, same data) ‚úÖ
   - Missing rows: INSERT ‚úÖ
5. Result: All cells exactly once in DB
```

**No IdempotencyComponent needed** - MERGE guarantees correctness even without batch-level tracking.

## Error Handling & Recovery

### flushTicks() Exception Behavior

When `flushTicks()` throws an exception, the error is handled by `AbstractBatchIndexer.processBatchMessage()`:

**Error Flow:**
```
1. flushTicks() throws SQLException (e.g., database connection lost)
   ‚Üì
2. processBatchMessage() catches Exception
   ‚Üì
3. log.warn("Failed to process batch...: {}: {}", batchId, e.getMessage()) + recordError()
   ‚Üì
4. NO topic.ack(msg) - batch remains unclaimed
   ‚Üì
5. Topic claimTimeout (300s) expires
   ‚Üì
6. Topic reassigns batch to another consumer (or same after recovery)
   ‚Üì
7. Batch reprocessed
   ‚Üì
8. MERGE prevents duplicates ‚úÖ
```

**Key Points:**
- ‚úÖ **Indexer continues running** - transient errors don't kill the service
- ‚úÖ **No data loss** - batch is redelivered after claimTimeout
- ‚úÖ **No duplicates** - MERGE handles redelivery idempotently
- ‚úÖ **Error tracking** - recordError() tracks failures for monitoring

**Exception Types:**

| Exception | Source | Behavior |
|-----------|--------|----------|
| `SQLException` | Database write failed | Transient error ‚Üí WARN + recordError + NO ACK ‚Üí Redelivery |
| `IOException` | Storage read failed | Transient error ‚Üí WARN + recordError + NO ACK ‚Üí Redelivery |
| `RuntimeException` | Coordinate conversion failed | Should never happen (valid metadata) ‚Üí Same as above |
| `InterruptedException` | Service shutdown | Re-thrown ‚Üí Service stops gracefully |

**flushTicks() Contract:**
```java
/**
 * Flushes ticks to database using MERGE for idempotency.
 * <p>
 * <strong>Exception Handling:</strong> Implementations should let exceptions
 * propagate to AbstractBatchIndexer for graceful error handling:
 * <ul>
 *   <li>SQLException: Database errors ‚Üí batch redelivered after claimTimeout</li>
 *   <li>RuntimeException: Logic errors ‚Üí batch redelivered after claimTimeout</li>
 * </ul>
 * <p>
 * <strong>IMPORTANT for Database Layer:</strong> Lower-level implementations
 * (e.g., IH2EnvStorageStrategy.writeTicks()) MUST use try-catch to rollback
 * transactions before re-throwing exceptions. This ensures the database
 * connection is returned to the pool in a clean state:
 * <pre>
 * // In Strategy.writeTicks():
 * try {
 *     // ... write operations ...
 *     conn.commit();
 * } catch (SQLException e) {
 *     try {
 *         conn.rollback();  // REQUIRED - clean connection!
 *     } catch (SQLException ex) {
 *         log.warn("Rollback failed: {}", ex.getMessage());
 *     }
 *     throw e;  // Re-throw for AbstractBatchIndexer
 * }
 * </pre>
 * <p>
 * The indexer service continues running even if this method throws an exception.
 * Failed batches are reassigned by the topic system for automatic recovery.
 *
 * @param ticks Ticks to flush (already buffered by TickBufferingComponent)
 * @throws SQLException if database write fails (transient - will be retried)
 * @throws RuntimeException if logic errors occur (should never happen)
 */
protected abstract void flushTicks(List<TickData> ticks) throws Exception;
```

### Failure Scenarios

| Failure Point | Error Type | Indexer Behavior | Batch Outcome |
|---------------|------------|------------------|---------------|
| Storage read fails | IOException | WARN + recordError | NO ACK ‚Üí Redelivery after 300s |
| Coordinate conversion fails | RuntimeException | WARN + recordError | NO ACK ‚Üí Redelivery after 300s |
| Database connection lost | SQLException | WARN + recordError | NO ACK ‚Üí Redelivery after 300s |
| MERGE constraint violation | SQLException | WARN + recordError | NO ACK ‚Üí Redelivery after 300s |
| Service shutdown | InterruptedException | Service stops | NO ACK ‚Üí Redelivery on restart |

**Recovery Guarantees:**
- ‚úÖ **Automatic recovery**: Topic reassignment after claimTimeout (300s)
- ‚úÖ **No data loss**: Failed batches are redelivered
- ‚úÖ **Idempotency**: MERGE prevents duplicates on redelivery
- ‚úÖ **Service availability**: Indexer continues processing other batches

### Graceful Shutdown

When the indexer receives a shutdown signal, it performs a **graceful shutdown** to minimize data loss and redelivery:

**Shutdown Flow:**
```
1. Service receives shutdown signal (Thread.interrupt())
   ‚Üì
2. Topic poll loop exits (isInterrupted() == true)
   ‚Üì
3. finally-block executes (GUARANTEED - even on interrupt)
   ‚Üì
4. TickBufferingComponent checks buffer size
   ‚Üì
5. If buffer not empty: flushAndAcknowledge() called
   ‚Üì
6. All buffered ticks written to database (MERGE)
   ‚Üì
7. Database commits transaction
   ‚Üì
8. Topic ACKs sent for all processed batches
   ‚Üì
9. Indexer terminates cleanly
```

**Implementation (AbstractBatchIndexer):**
```java
} finally {
    // Final flush of remaining buffered ticks (always executed, even on interrupt!)
    if (components != null && components.buffering != null 
        && components.buffering.getBufferSize() > 0) {
        try {
            flushAndAcknowledge();
        } catch (Exception e) {
            log.error("Final flush failed during shutdown");
            throw e;  // Service goes to ERROR state
        }
    }
}
```

**Shutdown Guarantees:**
- ‚úÖ **No buffered data loss**: All buffered ticks written before shutdown
- ‚úÖ **Clean acknowledgments**: All processed batches ACKed (no redelivery on restart)
- ‚úÖ **Minimized replay**: Only unclaimed batches need reprocessing on restart
- ‚úÖ **Transaction safety**: Database transaction committed before ACK
- ‚úÖ **Fast restart**: No need to reprocess already-flushed batches

**Exception During Final Flush:**

If the final flush fails (e.g., database unavailable):
```
1. log.error("Final flush failed during shutdown")
2. Exception thrown ‚Üí Service goes to ERROR state
3. Buffered ticks NOT ACKed ‚Üí Redelivered on restart
4. MERGE ensures no duplicates when reprocessed
```

This is intentional: Better to replay data than lose it or create duplicates.

**Comparison with Crash:**

| Scenario | Buffered Ticks | ACKs Sent | Restart Behavior |
|----------|----------------|-----------|------------------|
| **Graceful Shutdown** | Flushed to DB | Sent | Only unclaimed batches replayed |
| **Crash/Kill -9** | Lost from buffer | Not sent | All unclaimed batches replayed |
| **Final Flush Fails** | Lost from buffer | Not sent | All unclaimed batches replayed (same as crash) |

**Best Practice:** Use graceful shutdown (SIGTERM) instead of force kill (SIGKILL) to minimize replay overhead.

**Example - Database Connection Lost:**
```
1. Indexer processes batch_0 ‚Üí Success ‚Üí ACK ‚úÖ
2. Database connection lost
3. Indexer processes batch_1 ‚Üí SQLException in flushTicks()
   ‚Üí log.warn() + recordError()
   ‚Üí NO ACK
   ‚Üí Indexer continues to batch_2 ‚úÖ
4. After 300s: Topic reassigns batch_1
5. Database reconnects (automatic via connection pool)
6. Indexer reprocesses batch_1 ‚Üí Success ‚Üí ACK ‚úÖ
```

**Why NOT stop the indexer:**
- Database errors are often transient (connection timeout, disk full recovery)
- Other batches can still be processed successfully
- Competing consumers can take over failed batches
- Service restart would lose all buffered state

## Implementation

### 1. EnvironmentProperties Extension

**File:** `src/main/java/org/evochora/runtime/model/EnvironmentProperties.java`

Add methods for flat_index ‚Üí coordinate conversion:

```java
/**
 * Converts a flat index to coordinates.
 * <p>
 * This is the inverse operation of the linearization used by Environment.
 * Uses row-major order: flatIndex = coord[0] + coord[1]*stride[1] + coord[2]*stride[2] + ...
 * <p>
 * <strong>Performance:</strong> Strides are cached on first call for O(1) conversion.
 *
 * @param flatIndex The flat index to convert
 * @return Coordinate array with same length as worldShape
 * @throws IllegalArgumentException if flatIndex is negative or out of bounds
 */
public int[] flatIndexToCoordinates(int flatIndex) {
    if (flatIndex < 0) {
        throw new IllegalArgumentException("Flat index must be non-negative: " + flatIndex);
    }
    
    // Lazy-initialize strides cache
    if (strides == null) {
        strides = calculateStrides();
    }
    
    int[] coord = new int[worldShape.length];
    int remaining = flatIndex;
    
    for (int i = 0; i < worldShape.length; i++) {
        coord[i] = remaining / strides[i];
        remaining %= strides[i];
    }
    
    return coord;
}

/**
 * Calculates strides for flat index conversion.
 * <p>
 * Strides are cached to avoid repeated calculation.
 * Row-major order: stride[0]=1, stride[i]=stride[i-1]*shape[i-1]
 *
 * @return Strides array
 */
private int[] calculateStrides() {
    int[] s = new int[worldShape.length];
    s[0] = 1;
    for (int i = 1; i < worldShape.length; i++) {
        s[i] = s[i-1] * worldShape[i-1];
    }
    return s;
}

// Field for caching (add to class)
private int[] strides; // Cached strides for performance
```

**Rationale:**
- ‚úÖ Centralizes coordinate conversion logic (no duplication)
- ‚úÖ Environment-internal format remains encapsulated
- ‚úÖ O(1) conversion after first call (strides cached)
- ‚úÖ Can evolve with Environment without breaking indexer

### 2. Database Capability Interface

**File:** `src/main/java/org/evochora/datapipeline/api/resources/database/IEnvironmentDataWriter.java`

```java
package org.evochora.datapipeline.api.resources.database;

import org.evochora.datapipeline.api.contracts.CellState;
import org.evochora.datapipeline.api.resources.IMonitorable;

import java.sql.SQLException;
import java.util.List;

/**
 * Database capability for writing environment cell data.
 * <p>
 * Provides write operations for environment cells with dimension-agnostic schema.
 * Used by EnvironmentIndexer to persist cell states for HTTP API queries.
 * <p>
 * Extends {@link ISchemaAwareDatabase} - AbstractIndexer automatically calls
 * {@code setSimulationRun()} after run discovery to set the schema.
 * <p>
 * Implements {@link AutoCloseable} to enable try-with-resources pattern for
 * automatic connection cleanup.
 */
public interface IEnvironmentDataWriter extends ISchemaAwareDatabase, IMonitorable, AutoCloseable {
    
    /**
     * Creates the environment_ticks table idempotently.
     * <p>
     * Table schema (dimension-agnostic via BLOB):
     * <pre>
     * CREATE TABLE IF NOT EXISTS environment_ticks (
     *     tick_number BIGINT PRIMARY KEY,
     *     dimension_count INT NOT NULL,
     *     cells_blob BLOB NOT NULL
     * )
     * </pre>
     * <p>
     * Implementations should:
     * <ul>
     *   <li>Use CREATE TABLE IF NOT EXISTS (idempotent)</li>
     *   <li>Create spatial indexes for area queries</li>
     *   <li>Prepare MERGE statement for writeEnvironmentCells()</li>
     * </ul>
     *
     * @param dimensions Number of dimensions (e.g., 2 for 2D, 3 for 3D)
     * @throws SQLException if table creation fails
     */
    void createEnvironmentDataTable(int dimensions) throws SQLException;
    
    /**
     * Writes environment cells for multiple ticks using MERGE for idempotency.
     * <p>
     * All ticks are written in one JDBC batch with one commit for maximum performance.
     * Delegates to IH2EnvStorageStrategy for actual storage implementation.
     * <p>
     * Each tick is identified by tick_number and written with MERGE:
     * <ul>
     *   <li>If tick exists: UPDATE (overwrite with new data)</li>
     *   <li>If tick missing: INSERT</li>
     * </ul>
     * <p>
     * This ensures 100% idempotency even with topic redeliveries.
     * <p>
     * <strong>Performance:</strong> All ticks written in one JDBC batch with one commit.
     * This reduces commit overhead by ~1000√ó compared to per-tick commits.
     * <p>
     * <strong>Strategy-Specific:</strong> SingleBlobStrategy serializes cells to BLOB.
     * EnvironmentProperties passed for strategies that need coordinate conversion.
     *
     * @param ticks List of ticks with their cell data to write
     * @param envProps Environment properties for coordinate conversion (if needed by strategy)
     * @throws SQLException if database write fails
     */
    void writeEnvironmentCells(List<TickData> ticks, EnvironmentProperties envProps) 
            throws SQLException;
    
    /**
     * Closes the database wrapper and releases its dedicated connection back to the pool.
     * <p>
     * This method is automatically called when used with try-with-resources.
     * Implementations must ensure the connection is properly closed even if errors occur.
     */
    @Override
    void close();
}
```

### 3. Database Wrapper Implementation

**File:** `src/main/java/org/evochora/datapipeline/resources/database/EnvironmentDataWriterWrapper.java`

```java
package org.evochora.datapipeline.resources.database;

import org.evochora.datapipeline.api.contracts.CellState;
import org.evochora.datapipeline.api.resources.ResourceContext;
import org.evochora.datapipeline.api.resources.database.IEnvironmentDataWriter;
import org.evochora.datapipeline.utils.monitoring.SlidingWindowCounter;
import org.evochora.datapipeline.utils.monitoring.SlidingWindowPercentiles;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.SQLException;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicLong;

/**
 * Database-agnostic wrapper for environment data writing operations.
 * <p>
 * Extends {@link AbstractDatabaseWrapper} to inherit common functionality:
 * connection management, schema setting, error tracking, metrics infrastructure.
 * <p>
 * <strong>Performance Contract:</strong> All metrics use O(1) recording.
 */
class EnvironmentDataWriterWrapper extends AbstractDatabaseWrapper implements IEnvironmentDataWriter {
    private static final Logger log = LoggerFactory.getLogger(EnvironmentDataWriterWrapper.class);
    
    // Metrics (O(1) atomic operations)
    private final AtomicLong cellsWritten = new AtomicLong(0);
    private final AtomicLong batchesWritten = new AtomicLong(0);
    private final AtomicLong writeErrors = new AtomicLong(0);
    
    // Performance tracking (O(1) recording)
    private final SlidingWindowCounter cellThroughput;
    private final SlidingWindowCounter batchThroughput;
    
    // Latency tracking (O(1) recording)
    private final SlidingWindowPercentiles writeLatency;
    
    EnvironmentDataWriterWrapper(AbstractDatabaseResource db, ResourceContext context) {
        super(db, context);  // Parent handles connection, error tracking, metricsWindowSeconds
        
        // Note: metricsWindowSeconds inherited from parent (AbstractDatabaseWrapper)
        // Configuration hierarchy: URI parameter > Resource option > Default (60)
        
        // Initialize throughput trackers (same window as latency for consistency)
        this.cellThroughput = new SlidingWindowCounter(metricsWindowSeconds);
        this.batchThroughput = new SlidingWindowCounter(metricsWindowSeconds);
        this.writeLatency = new SlidingWindowPercentiles(metricsWindowSeconds);
    }
    
    @Override
    public void createEnvironmentDataTable(int dimensions) throws SQLException {
        try {
            database.doCreateEnvironmentDataTable(ensureConnection(), dimensions);
            log.debug("Environment ticks table created with {} dimensions", dimensions);
        } catch (Exception e) {
            log.error("Failed to create environment_ticks table with {} dimensions: {}", dimensions, e.getMessage());
            throw new SQLException("Failed to create environment_ticks table", e);
        }
    }
    
    @Override
    public void writeEnvironmentCells(List<TickData> ticks, EnvironmentProperties envProps) 
            throws SQLException {
        long startNanos = System.nanoTime();
        
        try {
            database.doWriteEnvironmentCells(ensureConnection(), ticks, envProps);
            
            // Calculate total cells across all ticks
            int totalCells = ticks.stream()
                .mapToInt(t -> t.getCellsList().size())
                .sum();
            
            // Update metrics
            cellsWritten.addAndGet(totalCells);
            batchesWritten.incrementAndGet();
            cellThroughput.recordSum(totalCells);  // O(1) - track cells/sec
            batchThroughput.recordCount();         // O(1) - track batches/sec (1 batch = all ticks)
            writeLatency.record(System.nanoTime() - startNanos);
            
        } catch (Exception e) {
            // Transient error - wrapper continues functioning
            writeErrors.incrementAndGet();
            log.warn("Failed to write {} ticks: {}", ticks.size(), e.getMessage());
            recordError("WRITE_FAILED", "Failed to write environment cells", 
                       "Ticks: " + ticks.size() + ", Error: " + e.getClass().getSimpleName());
            throw new SQLException("Failed to write environment cells", e);
        }
    }
    
    @Override
    protected void addCustomMetrics(Map<String, Number> metrics) {
        super.addCustomMetrics(metrics);  // Include parent metrics from AbstractDatabaseWrapper
        
        // Counters - O(1)
        metrics.put("cells_written", cellsWritten.get());
        metrics.put("batches_written", batchesWritten.get());
        metrics.put("write_errors", writeErrors.get());
        
        // Throughput - O(windowSeconds) = O(60) = O(constant)
        metrics.put("cells_per_second", cellThroughput.getRate());
        metrics.put("batches_per_second", batchThroughput.getRate());
        
        // Latency percentiles in milliseconds - O(windowSeconds √ó buckets) = O(constant)
        metrics.put("write_latency_p50_ms", writeLatency.getPercentile(50) / 1_000_000.0);
        metrics.put("write_latency_p95_ms", writeLatency.getPercentile(95) / 1_000_000.0);
        metrics.put("write_latency_p99_ms", writeLatency.getPercentile(99) / 1_000_000.0);
        metrics.put("write_latency_avg_ms", writeLatency.getAverage() / 1_000_000.0);
    }
}
```

### 4. Storage Strategy Architecture

**File:** `src/main/java/org/evochora/datapipeline/resources/database/h2/IH2EnvStorageStrategy.java`

```java
package org.evochora.datapipeline.resources.database.h2;

import org.evochora.datapipeline.api.contracts.TickData;
import org.evochora.runtime.model.EnvironmentProperties;

import java.sql.Connection;
import java.sql.SQLException;
import java.util.List;

/**
 * H2-specific strategy interface for storing environment data.
 * <p>
 * Different strategies trade off between storage size, query performance,
 * and write performance. This interface is H2-specific and cannot be used
 * with other database backends.
 * <p>
 * <strong>Rationale:</strong> Storage requirements for environment data can vary
 * dramatically based on environment size and tick count:
 * <ul>
 *   <li>Small runs (1000√ó1000, 10^6 ticks): ~15 GB with row-per-cell</li>
 *   <li>Large runs (10000√ó10000, 10^8 ticks): ~500 TB with row-per-cell ‚ùå</li>
 * </ul>
 * Different storage strategies enable different trade-offs without code changes.
 * <p>
 * <strong>Phase 14.3:</strong> Only write operations needed. Query methods
 * will be added in Phase 14.5 (HTTP API implementation).
 */
public interface IH2EnvStorageStrategy {
    
    /**
     * Creates the necessary schema (tables, indexes) for this storage strategy.
     * <p>
     * Must be idempotent - calling multiple times should be safe.
     *
     * @param conn Database connection (with autoCommit=false)
     * @param dimensions Number of spatial dimensions
     * @throws SQLException if schema creation fails
     */
    void createSchema(Connection conn, int dimensions) throws SQLException;
    
    /**
     * Writes environment data for multiple ticks using this storage strategy.
     * <p>
     * Implementation must commit the transaction on success and rollback on failure.
     *
     * @param conn Database connection (with autoCommit=false)
     * @param ticks List of ticks with cell data to write
     * @param envProps Environment properties for coordinate conversion
     * @throws SQLException if write fails
     */
    void writeTicks(Connection conn, List<TickData> ticks, 
                    EnvironmentProperties envProps) throws SQLException;
}
```

**Abstract Base Class:**

**File:** `src/main/java/org/evochora/datapipeline/resources/database/h2/AbstractH2EnvStorageStrategy.java`

```java
package org.evochora.datapipeline.resources.database.h2;

import com.typesafe.config.Config;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Abstract base class for H2 environment storage strategies.
 * <p>
 * Enforces constructor contract: All strategies MUST accept Config parameter.
 * <p>
 * Provides common infrastructure:
 * <ul>
 *   <li>Config options access (protected final)</li>
 *   <li>Logger instance (protected final)</li>
 * </ul>
 * <p>
 * <strong>Rationale:</strong> Ensures all strategies can be instantiated via reflection
 * with consistent constructor signature. The compiler enforces that subclasses call
 * super(options), preventing runtime errors from missing constructors.
 */
public abstract class AbstractH2EnvStorageStrategy implements IH2EnvStorageStrategy {
    
    protected final Logger log = LoggerFactory.getLogger(getClass());
    protected final Config options;
    
    /**
     * Creates storage strategy with configuration.
     * <p>
     * <strong>Subclass Requirement:</strong> All subclasses MUST call super(options).
     * The compiler enforces this.
     * 
     * @param options Strategy configuration (may be empty, never null)
     */
    protected AbstractH2EnvStorageStrategy(Config options) {
        this.options = java.util.Objects.requireNonNull(options, "options cannot be null");
    }
    
    // createSchema() and writeTicks() remain abstract - too strategy-specific
}
```

**Default Strategy Implementation:**

For Phase 14.3, implement `SingleBlobStrategy` (compact storage):

```java
package org.evochora.datapipeline.resources.database.h2;

/**
 * Stores environment data as one row per tick with cells serialized in a BLOB.
 * <p>
 * <strong>Storage:</strong> ~1 KB per tick (with compression)
 * <ul>
 *   <li>100√ó100 environment, 10% occupancy = 1000 cells/tick</li>
 *   <li>1000 cells √ó 20 bytes (protobuf) = 20 KB raw</li>
 *   <li>With compression: ~1-2 KB per tick</li>
 *   <li>10^6 ticks = 1-2 GB total ‚úÖ</li>
 * </ul>
 * <p>
 * <strong>Query Performance:</strong> Slower (must deserialize entire tick)
 * <p>
 * <strong>Write Performance:</strong> Excellent (one row per tick)
 * <p>
 * <strong>Best For:</strong> Large runs where storage size is critical.
 * <p>
 * <strong>Implementation Details:</strong> To be specified (table schema, blob format, etc.)
 */
public class SingleBlobStrategy extends AbstractH2EnvStorageStrategy {
    
    private final ICompressionCodec codec;
    private PreparedStatement mergeStatement;
    
    /**
     * Creates SingleBlobStrategy with optional compression.
     * 
     * @param options Config with optional compression block
     */
    public SingleBlobStrategy(Config options) {
        super(options);  // Required by abstract base class
        
        // Load compression codec (optional - defaults to NoneCodec if missing)
        this.codec = CompressionCodecFactory.create(options);
        
        log.debug("SingleBlobStrategy initialized with compression: {}", codec.getName());
    }
    
    @Override
    public void createSchema(Connection conn, int dimensions) throws SQLException {
        // Create table (dimension_count NOT stored - retrieved from metadata)
        conn.createStatement().execute(
            "CREATE TABLE IF NOT EXISTS environment_ticks (" +
            "  tick_number BIGINT PRIMARY KEY," +
            "  cells_blob BLOB NOT NULL" +
            ")"
        );
        
        // Create index (primary key index is automatic)
        conn.createStatement().execute(
            "CREATE INDEX IF NOT EXISTS idx_env_tick ON environment_ticks (tick_number)"
        );
        
        // Prepare reusable MERGE statement
        this.mergeStatement = conn.prepareStatement(
            "MERGE INTO environment_ticks (tick_number, cells_blob) " +
            "KEY (tick_number) VALUES (?, ?)"
        );
    }
    
    @Override
    public void writeTicks(Connection conn, List<TickData> ticks, 
                          EnvironmentProperties envProps) throws SQLException {
        try {
            for (TickData tick : ticks) {
                // Serialize cells to protobuf bytes with optional compression
                byte[] cellsBlob = serializeTickCells(tick);
                
                mergeStatement.setLong(1, tick.getTickNumber());
                mergeStatement.setBytes(2, cellsBlob);
                mergeStatement.addBatch();
            }
            
            // ONE executeBatch() for ALL ticks
            mergeStatement.executeBatch();
            
            // ONE commit() for ALL ticks - massive performance gain!
            conn.commit();
            
        } catch (SQLException e) {
            // Rollback to keep connection clean for pool reuse
            try {
                conn.rollback();
            } catch (SQLException rollbackEx) {
                log.warn("Rollback failed (connection may be closed): {}", rollbackEx.getMessage());
            }
            throw e;
        }
    }
    
    private byte[] serializeTickCells(TickData tick) throws SQLException {
        try {
            // Build CellStateList message
            CellStateList cellsList = CellStateList.newBuilder()
                .addAllCells(tick.getCellsList())
                .build();
            
            // Serialize to bytes with optional compression
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            try (OutputStream compressed = codec.wrapOutputStream(baos)) {
                cellsList.writeTo(compressed);
            }
            
            return baos.toByteArray();
            
        } catch (IOException e) {
            throw new SQLException("Failed to serialize cells for tick: " + tick.getTickNumber(), e);
        }
    }
}
```

**Note:** Alternative strategies (e.g., RowPerCellStrategy, RegionBasedStrategy) can be added later without changes to the core architecture.

### 5. H2 Database Implementation

**File:** `src/main/java/org/evochora/datapipeline/resources/database/H2Database.java`

**Add new abstract methods to AbstractDatabaseResource:**

```java
/**
 * Creates environment_ticks table with BLOB-based schema.
 * <p>
 * <strong>Transaction Handling:</strong> Must commit on success, rollback on failure.
 * <strong>Storage Strategy:</strong> Delegates to IH2EnvStorageStrategy for actual schema creation.
 * 
 * @param connection Database connection (from acquireDedicatedConnection)
 * @param dimensions Number of spatial dimensions (stored in table for validation)
 * @throws Exception if table creation fails
 */
protected abstract void doCreateEnvironmentDataTable(Object connection, int dimensions) 
        throws Exception;

/**
 * Writes environment cells for multiple ticks to database using MERGE for idempotency.
 * <p>
 * <strong>Transaction Handling:</strong> Must commit on success, rollback on failure.
 * <strong>Performance:</strong> All ticks written in one JDBC batch with one commit.
 * 
 * @param connection Database connection (from acquireDedicatedConnection)
 * @param ticks List of ticks with their cell data to write
 * @param envProps Environment properties for coordinate conversion
 * @throws Exception if write fails
 */
protected abstract void doWriteEnvironmentCells(Object connection, List<TickData> ticks,
        EnvironmentProperties envProps) throws Exception;
```

**Note:** Transaction contract (autoCommit=false, rollback on error) is already implemented in AbstractDatabaseResource and H2Database.

**Add storage strategy to H2Database:**

```java
// Field for storage strategy (loaded via reflection)
private IH2EnvStorageStrategy envStorageStrategy;

public H2Database(String name, Config options) {
    super(name, options);
    
    // ... existing HikariCP initialization ...
    
    // Load storage strategy via reflection (with options)
    if (options.hasPath("h2EnvironmentStrategy")) {
        Config strategyConfig = options.getConfig("h2EnvironmentStrategy");
        String strategyClassName = strategyConfig.getString("className");
        
        // Extract options for strategy (defaults to empty config if missing)
        Config strategyOptions = strategyConfig.hasPath("options")
            ? strategyConfig.getConfig("options")
            : ConfigFactory.empty();
        
        this.envStorageStrategy = createStorageStrategy(strategyClassName, strategyOptions);
        log.debug("Loaded environment storage strategy: {} with compression: {}", 
                 strategyClassName, strategyOptions.hasPath("compression.codec") 
                     ? strategyOptions.getString("compression.codec") 
                     : "none");
    } else {
        // Default: SingleBlobStrategy without compression
        Config emptyConfig = ConfigFactory.empty();
        this.envStorageStrategy = createStorageStrategy(
            "org.evochora.datapipeline.resources.database.h2.SingleBlobStrategy",
            emptyConfig
        );
        log.debug("Using default SingleBlobStrategy (no compression)");
    }
}

private IH2EnvStorageStrategy createStorageStrategy(String className, Config strategyConfig) {
    try {
        Class<?> strategyClass = Class.forName(className);
        
        // Try constructor with Config parameter first
        try {
            return (IH2EnvStorageStrategy) strategyClass
                .getDeclaredConstructor(Config.class)
                .newInstance(strategyConfig);
        } catch (NoSuchMethodException e) {
            // Fallback: no-args constructor (for simple strategies)
            return (IH2EnvStorageStrategy) strategyClass
                .getDeclaredConstructor()
                .newInstance();
        }
        
    } catch (ClassNotFoundException e) {
        throw new IllegalArgumentException(
            "Storage strategy class not found: " + className + 
            ". Make sure the class exists and is in the classpath.", e);
    } catch (ClassCastException e) {
        throw new IllegalArgumentException(
            "Storage strategy class must implement IH2EnvStorageStrategy: " + className, e);
    } catch (Exception e) {
        throw new IllegalArgumentException(
            "Failed to instantiate storage strategy: " + className + 
            ". Make sure the class has a public constructor(Config) or no-args constructor.", e);
    }
}

@Override
protected void doCreateEnvironmentDataTable(Object connection, int dimensions) throws Exception {
    Connection conn = (Connection) connection;
    
    // Delegate to storage strategy
    envStorageStrategy.createSchema(conn, dimensions);
}

// NOTE: Actual implementation is in SingleBlobStrategy class
// H2Database only delegates to the strategy

@Override
protected void doWriteEnvironmentCells(Object connection, List<TickData> ticks,
                                       EnvironmentProperties envProps) 
        throws Exception {
    Connection conn = (Connection) connection;
    
    // Delegate to storage strategy (strategy handles commit/rollback)
    envStorageStrategy.writeTicks(conn, ticks, envProps);
}

// NOTE: closeConnectionPool() remains unchanged - no strategy-specific cleanup needed
```

Update `getWrappedResource()` in AbstractDatabaseResource:

```java
@Override
public final IWrappedResource getWrappedResource(ResourceContext context) {
    String usageType = context.usageType();
    IWrappedResource wrapper = switch (usageType) {
        case "db-meta-write" -> new MetadataWriterWrapper(this, context);
        case "db-meta-read" -> new MetadataReaderWrapper(this, context);
        case "db-env-write" -> new EnvironmentDataWriterWrapper(this, context);  // NEW
        default -> throw new IllegalArgumentException(
                "Unknown database usage type: " + usageType + 
                ". Supported: db-meta-write, db-meta-read, db-env-write");
    };
    
    // Track wrapper for cleanup
    if (wrapper instanceof AutoCloseable) {
        activeWrappers.add((AutoCloseable) wrapper);
    }
    
    return wrapper;
}
```

### 5. EnvironmentIndexer Implementation

**File:** `src/main/java/org/evochora/datapipeline/services/indexers/EnvironmentIndexer.java`

```java
package org.evochora.datapipeline.services.indexers;

import com.google.protobuf.Message;
import com.typesafe.config.Config;
import org.evochora.datapipeline.api.contracts.CellState;
import org.evochora.datapipeline.api.contracts.SimulationMetadata;
import org.evochora.datapipeline.api.contracts.TickData;
import org.evochora.datapipeline.api.resources.IResource;
import org.evochora.datapipeline.api.resources.database.IEnvironmentDataWriter;
import org.evochora.runtime.model.EnvironmentProperties;

import java.sql.SQLException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Indexes environment cell states from TickData for efficient spatial queries.
 * <p>
 * This indexer:
 * <ul>
 *   <li>Reads TickData batches from storage (via topic notifications)</li>
 *   <li>Extracts CellState list (sparse - only non-empty cells)</li>
 *   <li>Converts flat_index to coordinates using EnvironmentProperties</li>
 *   <li>Writes to database with MERGE for 100% idempotency</li>
 *   <li>Supports dimension-agnostic schema (1D to N-D)</li>
 * </ul>
 * <p>
 * <strong>Resources Required:</strong>
 * <ul>
 *   <li>{@code storage} - IBatchStorageRead for reading TickData batches</li>
 *   <li>{@code topic} - ITopicReader for batch notifications</li>
 *   <li>{@code metadata} - IMetadataReader for simulation metadata</li>
 *   <li>{@code database} - IEnvironmentDataWriter for writing cells</li>
 * </ul>
 * <p>
 * <strong>Components Used:</strong>
 * <ul>
 *   <li>MetadataReadingComponent - waits for metadata before processing</li>
 *   <li>TickBufferingComponent - buffers cells for efficient batch writes</li>
 * </ul>
 * <p>
 * <strong>Competing Consumers:</strong> Multiple instances can run in parallel
 * using the same consumer group. Topic distributes batches across instances,
 * and MERGE ensures idempotent writes even with concurrent access.
 *
 * @param <ACK> The acknowledgment token type (implementation-specific)
 */
public class EnvironmentIndexer<ACK> extends AbstractBatchIndexer<ACK> {
    
    private final IEnvironmentDataWriter database;
    private EnvironmentProperties envProps;
    
    public EnvironmentIndexer(String name, Config options, Map<String, List<IResource>> resources) {
        super(name, options, resources);
        this.database = getRequiredResource("database", IEnvironmentDataWriter.class);
    }
    
    // Use default components: METADATA + BUFFERING
    // No override needed - AbstractBatchIndexer provides correct defaults
    
    @Override
    protected void prepareSchema(String runId) throws Exception {
        // Load metadata (provided by MetadataReadingComponent)
        SimulationMetadata metadata = components.metadata.getMetadata();
        
        // Extract environment properties for coordinate conversion
        this.envProps = extractEnvironmentProperties(metadata);
        
        // Create database table (idempotent)
        int dimensions = envProps.getWorldShape().length;
        database.createEnvironmentDataTable(dimensions);
        
        log.debug("Environment schema prepared: {} dimensions", dimensions);
    }
    
    @Override
    protected void flushTicks(List<TickData> ticks) throws Exception {
        if (ticks.isEmpty()) {
            log.debug("No ticks to flush");
            return;
        }

        // Write ALL ticks in one JDBC batch with one commit
        // Coordinate conversion happens in database layer
        database.writeEnvironmentCells(ticks, envProps);
        
        int totalCells = ticks.stream()
            .mapToInt(t -> t.getCellsList().size())
            .sum();
        
        log.debug("Flushed {} cells from {} ticks", totalCells, ticks.size());
    }
    
    /**
     * Extracts EnvironmentProperties from SimulationMetadata.
     */
    private EnvironmentProperties extractEnvironmentProperties(SimulationMetadata metadata) {
        // Extract world shape from metadata
        int[] worldShape = metadata.getEnvironment().getDimensionsList().stream()
            .mapToInt(SimulationMetadata.EnvironmentDimension::getSize)
            .toArray();
        
        // Extract topology
        boolean isToroidal = "TORUS".equalsIgnoreCase(
            metadata.getEnvironment().getTopology()
        );
        
        return new EnvironmentProperties(worldShape, isToroidal);
    }
    
}
```

## Configuration

**File:** `evochora.conf`

```hocon
resources {
  # Existing resources...
  
  index-database {
    className = "org.evochora.datapipeline.resources.database.H2Database"
    options {
      jdbcUrl = "jdbc:h2:${user.home}/evochora/data/evochora;MODE=PostgreSQL;AUTO_SERVER=TRUE"
      username = "sa"
      password = ""
      maxPoolSize = 10
      minIdle = 2
      metricsWindowSeconds = 60  # Time window for throughput/latency metrics (default: 60)
    }
  }
}

services {
  # Existing services...
  
  environment-indexer {
    className = "org.evochora.datapipeline.services.indexers.EnvironmentIndexer"
    
    resources {
      # Topic for batch notifications (competing consumer pattern)
      topic = "topic-read:batch-topic?consumerGroup=environment"
      
      # Storage for reading TickData batches
      storage = "storage-read:tick-storage"
      
      # Metadata for simulation configuration
      metadata = "db-meta-read:index-database"
      
      # Database for writing environment cells
      # URI parameters: metricsWindowSeconds (optional, overrides resource-level config)
      # Example with override: database = "db-env-write:index-database?metricsWindowSeconds=30"
      database = "db-env-write:index-database"
    }
    
    options {
      # Optional: explicit runId (otherwise discovered from storage)
      runId = ${?pipeline.services.runId}
      
      # Metadata component (standardized config names)
      metadataPollIntervalMs = 1000
      metadataMaxPollDurationMs = 300000
      
      # Buffering component (standardized config names)
      insertBatchSize = 1000    # Number of ticks to buffer before flush (all written in one JDBC batch + commit)
      flushTimeoutMs = 5000     # Flush after 5s even if buffer not full
      # Note: topicPollTimeoutMs automatically set to flushTimeoutMs
      # Performance: 1000 ticks √ó ~5000 cells = ~5M cells per commit (massive throughput!)
    }
  }
}
```

**Database Configuration (Storage Strategy):**

```hocon
resources {
  index-database {
    className = "org.evochora.datapipeline.resources.database.H2Database"
    options {
      jdbcUrl = "jdbc:h2:file:${user.home}/evochora/index"
      maxPoolSize = 10
      minIdle = 2
      
      # Storage strategy (optional - defaults to SingleBlobStrategy without compression)
      h2EnvironmentStrategy {
        className = "org.evochora.datapipeline.resources.database.h2.SingleBlobStrategy"
        options {
          # Compression is optional - if section missing, no compression used
          compression {
            enabled = true
            codec = "zstd"
            level = 3  # 1 (fastest) to 22 (best compression), default: 3
          }
        }
      }
      
      # Alternative strategies can be implemented later:
      # h2EnvironmentStrategy {
      #   className = "com.mycompany.custom.MyCustomStrategy"
      #   options { /* custom options */ }
      # }
    }
  }
}
```

**Competing Consumers Configuration:**

To run multiple indexer instances for higher throughput:

```hocon
services {
  environment-indexer-1 {
    className = "org.evochora.datapipeline.services.indexers.EnvironmentIndexer"
    resources {
      topic = "topic-read:batch-topic?consumerGroup=environment"  # Same group!
      storage = "storage-read:tick-storage"
      metadata = "db-meta-read:index-database"
      database = "db-env-write:index-database"
    }
    options { /* same as above */ }
  }
  
  environment-indexer-2 {
    className = "org.evochora.datapipeline.services.indexers.EnvironmentIndexer"
    resources {
      topic = "topic-read:batch-topic?consumerGroup=environment"  # Same group!
      storage = "storage-read:tick-storage"
      metadata = "db-meta-read:index-database"
      database = "db-env-write:index-database"
    }
    options { /* same as above */ }
  }
}
```

**Coordination:**
- **Topic:** Consumer group ensures each batch processed by exactly ONE instance
- **Database:** MERGE ensures idempotent writes (concurrent writes safe)
- **Schema:** CREATE TABLE IF NOT EXISTS ensures only one instance creates table

## Database Schema (SingleBlobStrategy)

**Table Structure:**

```sql
CREATE TABLE IF NOT EXISTS environment_ticks (
    tick_number BIGINT PRIMARY KEY,
    cells_blob BLOB NOT NULL
);
```

**Note:** dimension_count is NOT stored - it's retrieved from metadata table per run_id.
This avoids redundancy (all ticks in a run have same dimension count).

**Index:**

```sql
-- Primary key index on tick_number (automatic)
-- Enables fast tick-based queries
CREATE INDEX IF NOT EXISTS idx_env_tick ON environment_ticks (tick_number);
```

**BLOB Format:**

The `cells_blob` column contains a serialized `CellStateList` protobuf message:

```protobuf
// In tickdata_contracts.proto
message CellStateList {
  repeated CellState cells = 1;
}
```

**Serialization:**
```java
// Write: Serialize to bytes
CellStateList cellsList = CellStateList.newBuilder()
    .addAllCells(tick.getCellsList())
    .build();
byte[] blob = cellsList.toByteArray();
```

**Deserialization:**
```java
// Read: Parse from bytes
CellStateList cellsList = CellStateList.parseFrom(blob);
List<CellState> cells = cellsList.getCellsList();
```

Each CellState in the list contains:
- `flat_index` - Position in environment (converted to coordinates for queries)
- `molecule_type` - Type of molecule at this cell
- `molecule_value` - Value of molecule
- `owner_id` - Organism ID owning this cell

**Storage Characteristics:**

- **Compact:** One row per tick (~1-2 KB per tick with compression)
- **Sparse:** Only non-empty cells included in blob (same as protobuf)
- **Simple Primary Key:** tick_number only (no composite key needed)
- **MERGE-friendly:** Simple tick_number key enables efficient MERGE
- **Dimension-agnostic:** Blob format works for any number of dimensions

**Storage Size Comparison:**

| Strategy | Rows | Size per Tick | 10^6 Ticks | 10^8 Ticks |
|----------|------|---------------|------------|------------|
| **Row-Per-Tick** | 10^6 | ~1 KB | ~1 GB | ~100 GB |
| Row-Per-Cell | 10^9 | ~50 bytes | ~5 TB | ~500 TB |

**‚Üí Row-Per-Tick is 1000√ó more compact!**

## HTTP API Query Examples (Phase 14.5 - Future)

**Note:** With SingleBlobStrategy, HTTP API queries require deserialization:

```sql
-- Step 1: Read tick blob
SELECT cells_blob FROM environment_ticks WHERE tick_number = 1000;

-- Step 2: Deserialize blob in Java
List<CellState> cells = deserializeCellsBlob(blob);

-- Step 3: Filter cells by area in Java
List<CellState> filtered = cells.stream()
    .filter(cell -> {
        int[] coords = envProps.flatIndexToCoordinates(cell.getFlatIndex());
        return coords[0] >= 10 && coords[0] <= 20
            && coords[1] >= 30 && coords[1] <= 40;
    })
    .collect(Collectors.toList());
```

**Performance:**
- ‚úÖ Excellent for "get entire tick" queries
- ‚ö†Ô∏è Slower for small area queries (must deserialize entire tick)
- ‚úÖ Much better storage efficiency (1000√ó reduction)

**Single Cell Query:**

```sql
-- Get specific cell at specific tick (requires deserialization)
SELECT cells_blob FROM environment_ticks WHERE tick_number = 1000;

-- Then in Java: deserialize and find by flat_index or coordinates
```

**Tick Diff (future feature):**

```sql
-- Query both ticks
SELECT cells_blob FROM environment_ticks WHERE tick_number IN (1000, 2000);

-- Then in Java: deserialize both, compute diff
```

## Performance Considerations

### Memory Usage

**Indexer Memory (Write Path):**

Per Tick (100√ó100 2D, 50% occupied = 5,000 cells):
- CellState protobuf: ~20 bytes per cell
- CellStateList overhead: ~10 bytes
- **Total per tick:** ~100 KB

With insertBatchSize=1000:
- Buffer holds ~1000 ticks = 100 MB in memory
- Serialized to BLOB: ~1000 √ó 100 KB = 100 MB
- Acceptable for JVM heap (configure with -Xmx2G or more)

**API Server Memory (Read Path - Phase 14.5):**

Per Request for one tick:
- Must load entire BLOB into memory
- Deserialize to List<CellState>
- Filter in Java
- **Memory spike: ~100 KB to ~8 MB** depending on environment size
- High traffic ‚Üí multiple concurrent requests ‚Üí significant memory pressure

### Write Performance (Indexer)

**MERGE Batch (1,000 ticks with SingleBlobStrategy):**
- Serialization (CellStateList.toByteArray()): ~10 ms for 1000 ticks
- H2 Batch MERGE: ~50 ms for 1000 rows (SSD)
- Commit: ~5 ms
- **Total: ~70 ms = ~14,000 ticks/sec per indexer**
- **Cell throughput: ~14k ticks √ó 5k cells = 70M cells/sec** üöÄ

**Write Scalability:**
- Competing consumers: Linear scalability up to ~4-8 instances
- Bottleneck: Database connection pool (increase maxPoolSize)
- **Storage:** ~1 GB per 10^6 ticks (vs. 5 TB with row-per-cell)

### Read Performance (HTTP API - Phase 14.5)

**Critical Trade-off:** The storage savings come at the cost of read performance and memory.

**Read Path for Area Query:**
```
1. SQL: SELECT cells_blob FROM environment_ticks WHERE tick_number = ?
   ‚Üí ~5 ms (fast - single row lookup)
   
2. Deserialize BLOB: CellStateList.parseFrom(blob)
   ‚Üí ~50 ms for 5,000 cells (CPU-bound)
   
3. Filter in Java: Stream filter by coordinates
   ‚Üí ~10 ms for 5,000 cells
   
Total: ~65 ms per tick query
```

**Memory Impact on API Server:**

| Environment Size | Cells/Tick (10% occupied) | BLOB Size | API Server Memory |
|------------------|---------------------------|-----------|-------------------|
| 100√ó100 | 1,000 | ~20 KB | Negligible |
| 1000√ó1000 | 100,000 | ~2 MB | Acceptable |
| 2000√ó2000 | 400,000 | ~8 MB | ‚ö†Ô∏è Significant per request |
| 10000√ó10000 | 10,000,000 | ~200 MB | ‚ùå Problematic! |

**Implications for HTTP API (Phase 14.5):**
- ‚úÖ **Small environments (<1000√ó1000):** SingleBlobStrategy works well
- ‚ö†Ô∏è **Medium environments (2000√ó2000):** Consider caching, request throttling
- ‚ùå **Large environments (>5000√ó5000):** May need different strategy (RegionBasedStrategy)

**Read Scalability:**
- API server must handle BLOB deserialization + filtering per request
- High query load ‚Üí high CPU usage (deserialization) + memory (BLOB in RAM)
- Mitigation: Result caching, query rate limiting, CDN for static ticks

**The Design Trade-off:**

| Aspect | SingleBlobStrategy | RowPerCellStrategy |
|--------|-------------------|-------------------|
| **Database Storage** | ~1 GB (10^6 ticks) | ~5 TB (10^6 ticks) |
| **Write Performance** | Excellent (14k ticks/sec) | Good (slower commits) |
| **Read Performance** | Slow (deserialize entire tick) | Fast (SQL filtered) |
| **API Server Memory** | High (~200 MB for large tick) | Low (only filtered rows) |
| **API Server CPU** | High (deserialization) | Low (DB does filtering) |

**Recommendation:** SingleBlobStrategy is optimal for workstation/development use where:
- Storage is limited (laptop SSD)
- Write performance is critical (simulation can't wait)
- Read queries are infrequent (mostly offline analysis)

For production with high-traffic HTTP API, consider RegionBasedStrategy (future work).

**Performance Monitoring:**

Available metrics provide comprehensive performance visibility:

| Metric | Type | Purpose |
|--------|------|---------|
| `cells_written` | Counter | Total cells written (lifetime) |
| `batches_written` | Counter | Total DB batches written (lifetime) |
| `cells_per_second` | Throughput | Real-time write throughput (configurable window, default: 60s) |
| `batches_per_second` | Throughput | Real-time batch rate (configurable window, default: 60s) |
| `write_latency_p50_ms` | Latency | Median write latency |
| `write_latency_p95_ms` | Latency | 95th percentile latency |
| `write_latency_p99_ms` | Latency | 99th percentile latency |
| `write_errors` | Counter | Failed write operations |

**Configuration Hierarchy:**
1. **URI Parameter** (highest priority): `database = "db-env-write:indexDatabase?metricsWindowSeconds=30"`
2. **Resource Option**: `indexDatabase.options.metricsWindowSeconds = 60`
3. **Default**: 60 seconds

This allows per-service override of metrics window when needed (e.g., shorter window for high-frequency indexers).

**All metrics use O(1) recording** - no performance impact on critical path.

### Coordinate Conversion Performance

**EnvironmentProperties.flatIndexToCoordinates():**
- First call: O(dimensions) - calculate strides
- Subsequent calls: O(dimensions) - cached strides
- Typical: O(3) for 3D world = negligible overhead

## Testing Strategy

### Unit Tests

**File:** `src/test/java/org/evochora/datapipeline/services/indexers/EnvironmentIndexerTest.java`

```java
@Test
@Tag("unit")
void testExtractEnvironmentProperties_2D() {
    SimulationMetadata metadata = createMetadata(new int[]{100, 200}, "TORUS");
    
    EnvironmentProperties props = indexer.extractEnvironmentProperties(metadata);
    
    assertThat(props.getWorldShape()).isEqualTo(new int[]{100, 200});
    assertThat(props.isToroidal()).isTrue();
}

@Test
@Tag("unit")
void testExtractEnvironmentProperties_3D() {
    SimulationMetadata metadata = createMetadata(new int[]{10, 20, 30}, "BOUNDED");
    
    EnvironmentProperties props = indexer.extractEnvironmentProperties(metadata);
    
    assertThat(props.getWorldShape()).isEqualTo(new int[]{10, 20, 30});
    assertThat(props.isToroidal()).isFalse();
}

@Test
@Tag("unit")
void testFlushTicks_SingleTick() throws Exception {
    // Setup: Mock database
    IEnvironmentDataWriter mockDatabase = mock(IEnvironmentDataWriter.class);
    EnvironmentIndexer indexer = createIndexerWithMockDatabase(mockDatabase);
    
    // Create tick with 3 cells
    TickData tick = TickData.newBuilder()
        .setTickNumber(1000)
        .addCells(createCell(0, 1, 5))
        .addCells(createCell(1, 2, 10))
        .addCells(createCell(2, 3, 15))
        .build();
    
    // Execute
    indexer.flushTicks(List.of(tick));
    
    // Verify: database.writeEnvironmentCells() called once with list of ticks
    ArgumentCaptor<List<TickData>> ticksCaptor = ArgumentCaptor.forClass(List.class);
    ArgumentCaptor<EnvironmentProperties> envPropsCaptor = ArgumentCaptor.forClass(EnvironmentProperties.class);
    
    verify(mockDatabase, times(1)).writeEnvironmentCells(
        ticksCaptor.capture(),
        envPropsCaptor.capture()
    );
    
    // Verify single tick passed
    assertThat(ticksCaptor.getValue()).hasSize(1);
    assertThat(ticksCaptor.getValue().get(0).getTickNumber()).isEqualTo(1000L);
    assertThat(ticksCaptor.getValue().get(0).getCellsList()).hasSize(3);
}

@Test
@Tag("unit")
void testFlushTicks_MultipleTicks() throws Exception {
    IEnvironmentDataWriter mockDatabase = mock(IEnvironmentDataWriter.class);
    EnvironmentIndexer indexer = createIndexerWithMockDatabase(mockDatabase);
    
    // Create 3 ticks with different cell counts
    TickData tick1 = createTickWithCells(1000, 5);
    TickData tick2 = createTickWithCells(1001, 3);
    TickData tick3 = createTickWithCells(1002, 7);
    
    // Execute
    indexer.flushTicks(List.of(tick1, tick2, tick3));
    
    // Verify: writeEnvironmentCells() called ONCE with all 3 ticks
    verify(mockDatabase, times(1)).writeEnvironmentCells(anyList(), any());
    
    // Verify all ticks passed in one call
    ArgumentCaptor<List<TickData>> ticksCaptor = ArgumentCaptor.forClass(List.class);
    verify(mockDatabase).writeEnvironmentCells(ticksCaptor.capture(), any());
    
    assertThat(ticksCaptor.getValue()).hasSize(3);
    assertThat(ticksCaptor.getValue().get(0).getTickNumber()).isEqualTo(1000L);
    assertThat(ticksCaptor.getValue().get(1).getTickNumber()).isEqualTo(1001L);
    assertThat(ticksCaptor.getValue().get(2).getTickNumber()).isEqualTo(1002L);
}

@Test
@Tag("unit")
void testFlushTicks_EmptyTicks() throws Exception {
    IEnvironmentDataWriter mockDatabase = mock(IEnvironmentDataWriter.class);
    EnvironmentIndexer indexer = createIndexerWithMockDatabase(mockDatabase);
    
    // Create ticks with NO cells (empty environment)
    TickData emptyTick = TickData.newBuilder()
        .setTickNumber(1000)
        .build();  // No cells added
    
    // Execute
    indexer.flushTicks(List.of(emptyTick));
    
    // Verify: database IS called (but with empty tick - database can skip efficiently)
    verify(mockDatabase, times(1)).writeEnvironmentCells(anyList(), any());
}

@Test
@Tag("unit")
void testFlushTicks_MixedEmptyAndNonEmpty() throws Exception {
    IEnvironmentDataWriter mockDatabase = mock(IEnvironmentDataWriter.class);
    EnvironmentIndexer indexer = createIndexerWithMockDatabase(mockDatabase);
    
    // Mix: empty tick, non-empty, empty, non-empty
    TickData tick1 = TickData.newBuilder().setTickNumber(1000).build(); // Empty
    TickData tick2 = createTickWithCells(1001, 5);  // 5 cells
    TickData tick3 = TickData.newBuilder().setTickNumber(1002).build(); // Empty
    TickData tick4 = createTickWithCells(1003, 3);  // 3 cells
    
    // Execute
    indexer.flushTicks(List.of(tick1, tick2, tick3, tick4));
    
    // Verify: ONE call with all 4 ticks (database handles empty ticks efficiently)
    verify(mockDatabase, times(1)).writeEnvironmentCells(anyList(), any());
    
    // Verify all ticks passed (including empty ones)
    ArgumentCaptor<List<TickData>> ticksCaptor = ArgumentCaptor.forClass(List.class);
    verify(mockDatabase).writeEnvironmentCells(ticksCaptor.capture(), any());
    assertEquals(4, ticksCaptor.getValue().size());
}

@Test
@Tag("unit")
void testPrepareSchema() throws Exception {
    IEnvironmentDataWriter mockDatabase = mock(IEnvironmentDataWriter.class);
    EnvironmentIndexer indexer = createIndexerWithMockDatabase(mockDatabase);
    
    // Create metadata with 3 dimensions
    SimulationMetadata metadata = createMetadata(new int[]{10, 20, 30}, "TORUS");
    
    // Mock component to return metadata
    when(indexer.components.metadata.getMetadata()).thenReturn(metadata);
    
    // Execute
    indexer.prepareSchema("test-run-id");
    
    // Verify: createEnvironmentDataTable called with 3 dimensions
    verify(mockDatabase).createEnvironmentDataTable(3);
    
    // Verify: envProps cached correctly
    assertThat(indexer.envProps).isNotNull();
    assertThat(indexer.envProps.getWorldShape()).isEqualTo(new int[]{10, 20, 30});
}

// Helper methods
private CellState createCell(int flatIndex, int type, int value) {
    return CellState.newBuilder()
        .setFlatIndex(flatIndex)
        .setMoleculeType(type)
        .setMoleculeValue(value)
        .setOwnerId(0)
        .build();
}

private TickData createTickWithCells(long tickNumber, int cellCount) {
    TickData.Builder builder = TickData.newBuilder().setTickNumber(tickNumber);
    for (int i = 0; i < cellCount; i++) {
        builder.addCells(createCell(i, 1, i * 10));
    }
    return builder.build();
}

private SimulationMetadata createMetadata(int[] shape, String topology) {
    SimulationMetadata.Builder builder = SimulationMetadata.newBuilder();
    EnvironmentConfig.Builder envBuilder = EnvironmentConfig.newBuilder();
    envBuilder.setTopology(topology);
    for (int size : shape) {
        envBuilder.addDimensions(
            SimulationMetadata.EnvironmentDimension.newBuilder().setSize(size)
        );
    }
    builder.setEnvironment(envBuilder);
    return builder.build();
}
```

### Integration Tests

**File:** `src/test/java/org/evochora/datapipeline/services/indexers/EnvironmentIndexerIntegrationTest.java`

```java
@Test
@Tag("integration")
void testEnvironmentIndexerEndToEnd() throws Exception {
    // Setup: In-memory H2, fake topic, fake storage
    EnvironmentIndexer indexer = createIndexer();
    
    // Send: Metadata
    sendMetadata(runId, 2 /* dimensions */);
    
    // Send: 3 Batches with cells
    sendBatch(runId, "batch_0", 0, 100, createCells(100, 50)); // 50 cells
    sendBatch(runId, "batch_1", 100, 100, createCells(100, 75)); // 75 cells
    sendBatch(runId, "batch_2", 200, 100, createCells(100, 60)); // 60 cells
    
    // Wait for indexer to process
    await().atMost(5, SECONDS).until(() -> getCellCountInDb() == 185);
    
    // Verify: All cells in database with correct coordinates
    assertThat(getCellCountInDb()).isEqualTo(185);
    
    // Verify: Tick written to database with correct blob
    byte[] blob = queryTickBlob(tick=0);
    List<CellState> cells = deserializeBlob(blob);
    
    // Verify flat_index 25 is in the blob
    assertThat(cells).anyMatch(c -> c.getFlatIndex() == 25);
}

@Test
@Tag("integration")
void testMergeIdempotency() throws Exception {
    EnvironmentIndexer indexer = createIndexer();
    
    sendMetadata(runId, 2);
    sendBatch(runId, "batch_0", 0, 100, createCells(100, 50));
    
    await().until(() -> getCellCountInDb() == 50);
    
    // Send same batch again (Topic redelivery simulation)
    sendBatch(runId, "batch_0", 0, 100, createCells(100, 50));
    
    await().atMost(2, SECONDS).until(() -> getBatchesProcessed() == 2);
    
    // Verify: Still only 50 cells (MERGE prevents duplicates)
    assertThat(getCellCountInDb()).isEqualTo(50);
}

@Test
@Tag("integration")
void testCompetingConsumers() throws Exception {
    // Start 2 indexers with same consumer group
    EnvironmentIndexer indexer1 = createIndexer("environmentIndexer1");
    EnvironmentIndexer indexer2 = createIndexer("environmentIndexer2");
    
    indexer1.start();
    indexer2.start();
    
    sendMetadata(runId, 2);
    
    // Send 10 batches
    for (int i = 0; i < 10; i++) {
        sendBatch(runId, "batch_" + i, i * 100, 100, createCells(100, 50));
    }
    
    // Wait for all batches processed
    await().atMost(10, SECONDS).until(() -> getCellCountInDb() == 500);
    
    // Verify: All batches processed exactly once
    assertThat(getCellCountInDb()).isEqualTo(500);
    
    // Verify: Both indexers participated
    assertThat(indexer1.getBatchesProcessed() + indexer2.getBatchesProcessed()).isEqualTo(10);
}

@Test
@Tag("integration")
void testMultipleDimensions() throws Exception {
    // Test 1D, 2D, 3D, 4D
    for (int dimensions = 1; dimensions <= 4; dimensions++) {
        testWithDimensions(dimensions);
    }
}

private void testWithDimensions(int dimensions) throws Exception {
    EnvironmentIndexer indexer = createIndexer();
    
    int[] shape = new int[dimensions];
    Arrays.fill(shape, 10); // 10√ó10√ó10√ó...
    
    sendMetadata(runId, dimensions, shape);
    sendBatch(runId, "batch_0", 0, 10, createCellsForShape(shape, 5));
    
    await().until(() -> getCellCountInDb() == 5);
    
    // Verify: Schema has correct number of pos_i columns
    assertThat(getPositionColumnCount()).isEqualTo(dimensions);
}
```

### Test Utilities

```java
private List<CellState> createCells(int count, int nonEmpty) {
    List<CellState> cells = new ArrayList<>();
    for (int i = 0; i < nonEmpty; i++) {
        cells.add(CellState.newBuilder()
            .setFlatIndex(i)
            .setMoleculeType(1)
            .setMoleculeValue(42)
            .setOwnerId(1)
            .build());
    }
    return cells;
}
```

## Future Enhancements

### Partitioning (Not in Phase 14.3)

**Challenge:** H2 does not support native table partitioning or UNION VIEW optimization.

**Current Approach:** Single table per schema (run ID). Works for:
- Development/testing (smaller runs)
- Moderate production runs (<10^7 ticks)

**Future Options (when needed):**
1. **Application-level partitioning:** Multiple tables, indexer routes based on tick range
2. **Database migration:** PostgreSQL with native PARTITION BY RANGE
3. **Hybrid:** Old ticks archived to compressed Protobuf Binary

**Decision:** Defer until performance monitoring shows need. Initial implementation validates architecture with smaller runs.

### Additional Query Patterns

**Current:** Area queries only (tick + spatial range)

**Future possibilities:**
- Time-series queries: "Cell (x,y) over time"
- Change detection: "Modified cells between tick X and Y"
- Pattern matching: "Cells matching criteria"

**Decision:** Implement when HTTP API requirements clarify. Schema supports these patterns.

## Success Criteria Verification

Upon completion, verify:

1. ‚úÖ AbstractDatabaseWrapper supports URI parameters (all database wrappers benefit)
2. ‚úÖ EnvironmentProperties.flatIndexToCoordinates() with O(1) cached strides
3. ‚úÖ IEnvironmentDataWriter interface (delegates to strategy)
4. ‚úÖ EnvironmentDataWriterWrapper with O(1) throughput/latency metrics
5. ‚úÖ IH2EnvStorageStrategy interface (createSchema, writeTicks)
6. ‚úÖ SingleBlobStrategy implementation (BLOB-based storage)
7. ‚úÖ H2Database: Strategy loading via reflection + delegation
8. ‚úÖ H2Database MERGE implementation (100% idempotency on tick_number)
9. ‚úÖ EnvironmentIndexer extends AbstractBatchIndexer (METADATA + BUFFERING)
10. ‚úÖ Only non-empty cells stored (sparse in BLOB)
11. ‚úÖ Competing consumers work (multiple indexer instances)
12. ‚úÖ Unit tests pass (serialization, schema creation, MERGE idempotency)
13. ‚úÖ Integration tests pass (end-to-end, idempotency, competing consumers, dimensions)
14. ‚úÖ Storage efficiency: ~1 GB for 10^6 ticks (1000√ó better than row-per-cell)

## Implementation Notes

**No backward compatibility** - this is a new indexer.

**Implementation order:**
1. ‚úÖ AbstractDatabaseWrapper URI parameter support (benefits all database wrappers)
2. ‚úÖ Transaction contract (AbstractDatabaseResource JavaDoc + H2Database rollback in doInsertMetadata)
3. ‚úÖ CellStateList protobuf message (wrapper for repeated CellState)
4. EnvironmentProperties.flatIndexToCoordinates() (with tests)
5. IH2EnvStorageStrategy interface (minimal: createSchema, writeTicks)
6. AbstractH2EnvStorageStrategy base class (Config + Logger infrastructure)
7. SingleBlobStrategy implementation (default - compact storage with optional compression)
8. IEnvironmentDataWriter interface
9. EnvironmentDataWriterWrapper (with log.warn + recordError for transient errors)
10. H2Database: Strategy loading via reflection + delegation
11. AbstractDatabaseResource.getWrappedResource() (add "db-env-write" case)
12. EnvironmentIndexer
13. Integration tests (including error recovery scenarios, strategy switching, compression)
14. Configuration update

**Testing priorities:**
1. Coordinate conversion correctness (all dimensions)
2. MERGE idempotency (duplicate batches)
3. Competing consumers (no data loss)
4. Schema adaptation (1D to N-D)
5. Error recovery (rollback, redelivery)

**Transaction Contract:**
- All `doWrite*()` methods in `H2Database` MUST use try-catch with rollback
- All wrappers use `log.warn()` + `recordError()` for transient errors (database writes)
- Fatal errors (schema creation) still use `log.error()` without `recordError()`
- Connection pool receives only clean connections (committed or rolled back)

---

**Phase Tracking:**
- Phase 14.2: Indexer Foundation ‚úÖ Completed
- Phase 14.3: Environment Indexer ‚è≥ This Document
- Phase 14.4: Organism Indexer (future)
- Phase 14.5: HTTP API (future)

